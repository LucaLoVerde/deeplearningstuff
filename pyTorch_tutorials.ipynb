{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# pyTorch tutorials\n",
    "Taken from the official website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Essentials\n",
    "### Creating empty tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 3.5831e-19,  1.4013e-45,  2.1390e-19],\n",
      "        [ 4.5740e-41,  0.0000e+00,  0.0000e+00],\n",
      "        [ 4.2803e-28, -1.5849e+29,  4.2804e-28]])\n",
      "tensor([[0.7789, 0.7275, 0.5827],\n",
      "        [0.5068, 0.0809, 0.3517],\n",
      "        [0.4566, 0.9399, 0.5542],\n",
      "        [0.1592, 0.8336, 0.0607],\n",
      "        [0.4987, 0.9373, 0.2256]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "vuoto1 = torch.empty(5, 3)  # creates a 5-by-3 empty tensor\n",
    "print(vuoto1)\n",
    "\n",
    "random1 = torch.rand(5, 3)  # creates a 5-by-3 random tensor\n",
    "print(random1)\n",
    "\n",
    "zeros1 = torch.zeros(10, 2)  # creates a 10-by-2 zeroed tensor\n",
    "print(zeros1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Constructing a tensor from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([10.0000, 20.3000,  2.0000])\n",
      "tensor([10, 20,  2])\n",
      "tensor([10.0000, 20.3000,  2.0000], dtype=torch.float64)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# To construct a tensor from data, we can do:\n",
    "from_data1 = torch.tensor([10, 20.3, 2])\n",
    "print(from_data1)\n",
    "\n",
    "# We can specify the type of data we want\n",
    "from_data2 = torch.tensor([10, 20.3, 2], dtype=torch.long)\n",
    "print(from_data2)\n",
    "\n",
    "from_data3 = torch.tensor([10, 20.3, 2], dtype=torch.double)\n",
    "print(from_data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Converting other data types to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[-2.6816e+154, -2.6816e+154],\n",
      "        [ 9.1503e+199,   2.6355e+92],\n",
      "        [ 6.2920e+233,   6.0934e-13],\n",
      "        [ 4.9428e+160,  4.8256e+276],\n",
      "        [ 1.1132e+171,  2.6469e+180],\n",
      "        [ 7.6876e+170,  2.5172e+180],\n",
      "        [ 4.6568e+164,  4.9775e+151],\n",
      "        [  2.3352e+30,  3.4351e+228],\n",
      "        [ 1.0258e+200,  3.4521e+175],\n",
      "        [ 7.0636e-308,  4.9407e-324]], dtype=torch.float64)\n",
      "tensor([ 0.5048,  1.2861, -1.5151])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 2])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Or it is possible to \"convert\" and reuse other tensors as well\n",
    "from_data4 = from_data3.new_empty(10, 2)\n",
    "print(from_data4)\n",
    "from_data5 = torch.randn_like(from_data1, dtype=torch.float)\n",
    "print(from_data5)\n",
    "\n",
    "# To obtain tensor size:\n",
    "print(from_data5.size())\n",
    "print(from_data4.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Operations on tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[ 1.9989,  0.2214,  1.2303],\n",
      "        [-0.8960, -0.3022, -1.1373],\n",
      "        [-0.1791,  1.3076,  0.3348],\n",
      "        [-0.0956,  0.3738,  0.4948],\n",
      "        [ 0.6427,  1.1976,  0.3401]])\n",
      "tensor([[ 1.9989,  0.2214,  1.2303],\n",
      "        [-0.8960, -0.3022, -1.1373],\n",
      "        [-0.1791,  1.3076,  0.3348],\n",
      "        [-0.0956,  0.3738,  0.4948],\n",
      "        [ 0.6427,  1.1976,  0.3401]])\n",
      "tensor([[ 1.9989,  0.2214,  1.2303],\n",
      "        [-0.8960, -0.3022, -1.1373],\n",
      "        [-0.1791,  1.3076,  0.3348],\n",
      "        [-0.0956,  0.3738,  0.4948],\n",
      "        [ 0.6427,  1.1976,  0.3401]])\n",
      "tensor([0.7275, 0.0809, 0.9399, 0.8336, 0.9373])\n",
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "\n",
      "tensor([[[ 0.0109, -0.9613],\n",
      "         [ 3.1196,  0.3278]],\n",
      "\n",
      "        [[ 0.7942, -0.9987],\n",
      "         [ 0.0810,  0.4879]],\n",
      "\n",
      "        [[ 0.2968, -0.1284],\n",
      "         [ 0.2168,  1.2693]],\n",
      "\n",
      "        [[ 0.6893, -0.8886],\n",
      "         [ 0.0332, -1.3138]]]) torch.Size([4, 2, 2])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "random2 = torch.randn_like(random1)\n",
    "print(random1 + random2)  # Sum of tensors\n",
    "print(torch.add(random1, random2))  # Another way, explicit method call\n",
    "risultato_somma = torch.empty_like(random1)  # Another way, involving pre-initializing the result storage tensor\n",
    "torch.add(random1, random2, out=risultato_somma)\n",
    "print(risultato_somma)\n",
    "random3 = torch.randn_like(random1)\n",
    "random3.add_(random2)  # In place addition, will add random2 to random3, and reassign the result\n",
    "\n",
    "# Tensors support all the NumPy-like indexing facilities:\n",
    "print(random1[:, 1])\n",
    "\n",
    "# Also to reshape, it's possible to use the torch.view() methods:\n",
    "x1 = torch.randn(4, 4)  # 4 by 4 random tensor, size == 16\n",
    "y1 = x1.view(16)  # now, the same elements are arranged as a 16 (by 1) tensor\n",
    "z1 = x1.view(-1, 8)  # ask to put 8 elements on the second dimension and to infer how many it needs to put in the first (-1)\n",
    "z2 = x1.view(-1, 2, 2)  # now let's ask to do a 3-D tensor with 2 elements on the second dimension and 2 elements on the third dimension, let's allow torch to infer how many elementds it should put on the remaining (1st) dimension\n",
    "print(x1.size(), y1.size(), z1.size())\n",
    "print()\n",
    "print(z2, z2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The command ```torch.view()``` can be used to resize/reshape tensors (by selecting the desired elements) by asking it to \n",
    "rearrange the elements so that they fit in the requested size/dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([0.9488])\n",
      "0.9488030672073364\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# For a singleton tensor, we can get the value as a normal number with:\n",
    "singleton1 = torch.randn(1)\n",
    "print(singleton1)\n",
    "print(singleton1.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can use the ```torch.item()``` method to extract the actual element from the tensor, as seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Numpy integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "a1 = torch.ones(10)\n",
    "print(a1)\n",
    "a1_numpy = a1.numpy()  # To convert to a NumPy array\n",
    "print(a1_numpy)\n",
    "\n",
    "# Let's see what happens with:\n",
    "a1.add_(1)\n",
    "print(a1)\n",
    "print(a1_numpy)  # it carries the change from the tensor to the NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any operator which mutates directly the original variable (like in the example above) is post-fixed with a \"_\" symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# And the contrary is doable, too\n",
    "numpy_a1 = np.ones(10)\n",
    "from_numpy_a1 = torch.from_numpy(numpy_a1)\n",
    "np.add(numpy_a1, 1, out=numpy_a1)\n",
    "print(numpy_a1)\n",
    "print(from_numpy_a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CUDA tensors\n",
    "To take advantage of a GPU with CUDA support, we should \"address\" the tensors to the correct device, like shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "CUDA not available on this machine/setup\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x1, device=device)  # directly create a tensor on GPU\n",
    "    x1 = x1.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x1 + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n",
    "else:\n",
    "    print(\"CUDA not available on this machine/setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: automatic differentiation\n",
    "By setting a tensor's attribute ```.required_grad``` as ```True```, the operations done on the tensor will be tracked \n",
    "automatically. To halt the tracking, we can call the ```.detach()``` method and future operations on the tensor will not\n",
    "be tracked. To temporarily perform untracked operations on a tensor that is being tracked, we can wrap the computations\n",
    "within a ```with torch.no_grad():``` block.\n",
    "Every tensor has a a reference to a function that has created the tensor, ```.grad_fn``` (except for user-made tensors, \n",
    "whose ```grad_fn``` is ```None```.\n",
    "All of this has to do with the fact that we are trying to find the optimal weights for several layers of neurons. In order\n",
    "to efficiently do this, **the cost function needs to be properly back-propagated from the output layer all the way to the\n",
    "input**. Automatic differentiation is an efficient algorithm to perform this task, and keeping track of every operation\n",
    "performed on our tensors is essential for it to work properly.\n",
    "Let's try this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x127cb3b90>\n",
      "None\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True) tensor(27., grad_fn=<MeanBackward0>)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "# Let's perform an operation on this tensor.\n",
    "y = x + 2\n",
    "print(y)\n",
    "# Given that the tensor y was created through an operation, it has a grad_fn attached.\n",
    "print(y.grad_fn)\n",
    "print(x.grad_fn)  # But a user-made tensor, on the other hand, has None as it's grad_fn\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(x, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Networks\n",
    "The ```nn``` module depends on ```autograd``` to define models and differentiate them. This module's purpose is to contain\n",
    "layers and a method ```forward(input)``` that returns the ```output```.\n",
    "Let's now define an example neural network (```convnet``` I think, used to classify handwritten digits?)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # This network takes 1 input image channel, has 6 output channels and a 3x3 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        # Don't have clues about this. Maybe it's just me?\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window, they say\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square we can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This defines the architecture of the net. We have to define the ```forward``` function, while the ```backward``` function will be \n",
    "automatically defined by ```autograd```.\n",
    "We can then obtain the list of learnable parameters using the following syntax:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's try a random 32x32 input. This net expects an input size of 32x32."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[-0.1405,  0.0683,  0.0055, -0.0666,  0.0289, -0.1163, -0.0818,  0.0975,\n",
      "         -0.1403,  0.0360]], grad_fn=<AddmmBackward>)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "my_input = torch.randn(1, 1, 32, 32)\n",
    "out = net(my_input)\n",
    "print(out)\n",
    "\n",
    "net.zero_grad()  # Let's zero the gradient buffers\n",
    "out.backward(torch.randn(1, 10))  # and reset the backprops randomly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recap\n",
    "* ```torch.Tensor``` is a multi-dimensional array supporting ```autograd``` operations like ```backwards()```; it also \n",
    "**holds the gradient** with regard to the tensor itself.\n",
    "* ```(torch.)nn.Module``` contains the neural network module, convenient way of encapsulating the parameters with various\n",
    "helpers.\n",
    "* ```(torch.)nn.Parameter``` is a kind of tensor automatically registered as a parameter when assigned as an attribute\n",
    "to a module.\n",
    "* ```autograd.Function``` implements ```forward``` and ```backward``` definitions of an autograd operation. Every tensor\n",
    "operation creates at least a single function node that connects to functions that created a tensor and encodes its history.\n",
    "operation creates at least a single function node that connects to functions that created a tensor and encodes its history.\n",
    "\n",
    "# Loss Function\n",
    "A loss function takes a (output, target) pair of inputs and computes a value that estimates how far away the output is\n",
    "from the target. The package ```nn``` contains several different loss functions: a simple loss is ```nn.MSELoss``` which\n",
    "computes the mean-squared error between the input and the target.\n",
    "Example:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor(1.0317, grad_fn=<MseLossBackward>)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "output = net(my_input)\n",
    "target = torch.randn(10)  # dummy target\n",
    "target = target.view(1, -1)  # make it the same shape as the output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we followed loss in the ```backward``` direction using its ```.grad_fn``` attribute, we will walk through a graph of \n",
    "all the computations from ```input``` to ```loss``` (the last being performed). Something similar to:\n",
    "\n",
    "```input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When calling ```loss.backward()```, the whole graph is differentiated with regard to the ```loss``` and all the tensors \n",
    "in the graph that have ```requires_grad=True``` will have their ```.grad``` tensor accumulated with the gradient. Let's\n",
    "try this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<MseLossBackward object at 0x127d16510>\n",
      "<AddmmBackward object at 0x127d16710>\n",
      "<AccumulateGrad object at 0x127d16510>\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(loss.grad_fn)  # this would be MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # this would be Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # and this would be reLU"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Backprop\n",
    "In order to backpropagate the error, we just have to call ```loss.backwards()```."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0033, -0.0021,  0.0218,  0.0212,  0.0193, -0.0135])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "net.zero_grad()  # Let's zero all the gradients again\n",
    "\n",
    "print(\"conv1.bias.grad before backward\")\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"conv1.bias.grad after backward\")\n",
    "print(net.conv1.bias.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Updating the weights\n",
    "Now that we have computed the loss function, we can deal with how to update the weights. The simplest update rule\n",
    "in practice is the **Stochastic Gradient Descent (SGD)**:\n",
    "```weight = weight - learning_rate * gradient```. Let's try this."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This works, but several other update rules (such as SGD, Nesterov-SGD, Adam, RMSProp) are included in the ```torch.optim```\n",
    "package. Let's try."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Let's create an optimizer!\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# And then we would put the following in our training loop:\n",
    "optimizer.zero_grad()\n",
    "output = net(my_input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()  # Actual update"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training a classifier\n",
    "boiadeh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}