{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Chapter 2. Neural Networks\n",
    "### Intro to neural networks\n",
    "* Information occurs mathematically, over simple elements called **neurons**\n",
    "* Neurons are connected and exchange signals (i.e. numbers) between each other through links\n",
    "* Each link has a **weight**, determining how information is processed as it passes through\n",
    "* Each neuron has an internal state, determined by all the incoming links\n",
    "* Each neuron has an **activation function** which determines the output signal as a function of its state\n",
    "\n",
    "The **architecture** of a neural network defines what type of connectivity the units have (i.e. feedforward, recurrent, multi-\n",
    "or single-layered, etc.), the number of layers and of neurons within each layer.\n",
    "The **learning** describes how the network adapt and improves; common tools are _gradient descent_ and _backpropagation_.\n",
    "\n",
    "### Intro to neurons\n",
    "Mathematically, a neuron is defined as:\n",
    "```\n",
    "y = f(sum(x[:] * w[:]) + b)\n",
    "```\n",
    "where ```x``` represents the inputs, and ```w``` represents the weights. ```b``` represents the _bias_ and its input is \n",
    "always 1.\n",
    "\n",
    "### Intro to layers\n",
    "In a neural network, the input layer represents the dataset and the initial conditions: if the net deals with grayscale \n",
    "images, the units in the first layer will represent the pixel intensity. The output layer can have more than one neuron:\n",
    "usually we find one unit per possible answer/class.\n",
    "\n",
    "### Multi-layer networks\n",
    "Single layer networks can only classify linearly separable classes, but by adding **hidden layers** we can surpass this\n",
    "limitation. Another condition for multi-layer network to classify linearly-inseparable classes is that their **activation\n",
    "function must not be linear**.\n",
    "\n",
    "### Different types of activation function\n",
    "The most common activation functions are the following:\n",
    "* ```f(a) = a``` - **identity function**\n",
    "* ```f(a) = [1 if a >= 0, otherwise 0]``` - **threshold activity function**\n",
    "* ```f(a) = 1 / (1 + exp(-a))``` - **logistic function**, or **logistic sigmoid** (0, 1)\n",
    "* ```f(a) = 2 / (1 + exp(-a)) = (1 - exp(-a)) / (1 + exp(-a))``` - **bipolar sigmoid** (-1, 1)\n",
    "* ```f(a) = (exp(a)-exp(-a)) / (exp(a) + exp(-a)) = (1 - exp(-2 * a)) / (1 + exp(-2 * a))``` - **hyperbolic tangent**, tanh (-1, 1)\n",
    "* ``` f(a) = [a if a >= 0, otherwise 0]``` - **rectified linear unit**, **ReLU** (0, Inf)\n",
    "\n",
    "### Example\n",
    "Let's build a very simple net, one hidden layer with two neurons, and single input and output neurons. It will support\n",
    "the idea that under the _Universal Approximation Theorem_ any continuous function on compact subsets of Rn can be \n",
    "approximated by a neural network with at least one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weight_value = 1000\n",
    "\n",
    "bias_value_1 = 5000\n",
    "\n",
    "bias_value_2 = -5000\n",
    "\n",
    "plt.axis([-10, 10, -1, 10])\n",
    "\n",
    "print(\"The step function starts at {0} and ends at {1}\".format(-bias_value_1 / weight_value, -bias_value_2 / weight_value))\n",
    "\n",
    "inputs = np.arange(-10, 10, 0.01)\n",
    "outputs = list()\n",
    "\n",
    "for x in inputs:\n",
    "    y1 = 1.0 / (1.0 + np.exp(-weight_value * x - bias_value_1))\n",
    "    y2 = 1.0 / (1.0 + np.exp(-weight_value * x - bias_value_2))\n",
    "    \n",
    "    w = 7\n",
    "    \n",
    "    y = y1 * w - y2 * w\n",
    "    \n",
    "    outputs.append(y)\n",
    "plt.plot(inputs, outputs, lw=2, color='black')\n",
    "plt.show(block=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Pyplot backends and settings\n",
    "Importing ```matplotlib``` and calling the method ```matplotlib.use()``` allows to change pyplot backend.\n",
    "* **default** DON'T KNOW \n",
    "* **'WebAgg'** opens a new browser tab, interactive\n",
    "* **'Qt5Agg'** new window, interactive (use ```plt.show(block=False)```!!!)\n",
    "\n",
    "Beware: after setting the backend to ```Qt5Agg``` (or any interactive one), to have inline plots in the notebook again\n",
    "the Jupyter kernel must be restarted before disabling the ```matplotlib.use()``` call.\n",
    "\n",
    "### Training neural networks\n",
    "We will now explore how the training procedure works on a very simple, 1-layer network using an algorithm called\n",
    "**gradient descent**, then extending the example to a deep forward network using **backpropagation**. During training, we\n",
    "use these tools with the aim of decreasing the network prediction error as much as possible. It is a minima finding problem.\n",
    "\n",
    "### Linear regression\n",
    "In vector notation, the output of a linear regression is a single value, ```y```, and is equal to the dot product of the\n",
    "input values ```x``` and the weights ```w```. Linear regression **is a special case of a neural network** with a single\n",
    "unit and identity activation function. \n",
    "\n",
    "Gradient descent works as follows:\n",
    "1. Initialize the weights ```w``` with random values\n",
    "2. Repeat:\n",
    "    * Compute the MSE for all the samples of the training set (```(sum(x * w - t) ^ 2) / n```)\n",
    "    * Compute the derivative of the MSE for each weight, update the weights to minimize it, until MSE is below threshold\n",
    "    \n",
    "The learning rate defines the ratio by which the weight adjusts as new data arrives. \n",
    "\n",
    "### Logistic regression\n",
    "Same as above, but with a different activation function (logistic sigmoid activation).\n",
    "\n",
    "### Backpropagation\n",
    "For a 1-layer network, updating the weights using gradient descent is straightforward since we can compare the output\n",
    "with the target and update the weights. In multi-layer networks, we could only do this for the weights that connect\n",
    "the the final hidden layer to the output layer, because we don't have target values for the intermediate representations.\n",
    "\n",
    "In this case, what we do is calculate the error in the final hidden layer and estimate what it would have been in the \n",
    "previous layer. Then, we propagate the error back from the last layer to the first.\n",
    "\n",
    "### Code example: net for the XOR function\n",
    "XOR is a linearly inseparable function, so we will use it to show a net with a hidden layer. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# noinspection PyShadowingNames\n",
    "def tanh(x: float):\n",
    "    return (1.0 - np.exp(-2 * x)) / (1.0 + np.exp(-2 * x))\n",
    "\n",
    "# noinspection PyShadowingNames\n",
    "def tanh_deriv(x: float):\n",
    "    return (1 + tanh(x)) * (1 - tanh(x))\n",
    "\n",
    "# noinspection PyPep8Naming,PyShadowingNames,PyMethodMayBeStatic\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    NeuralNetwork represents a simple feedforward network, consisting of an architecture represented by an array whose \n",
    "    length defines the number of layers and the elements the number of units in each layer. Implements methods to fit the\n",
    "    data, to make a prediction and to plot the results.\n",
    "    \"\"\"\n",
    "    def __init__(self, net_arch: list):\n",
    "        \"\"\"\n",
    "        :param net_arch: Architecture of the net, as an array of number of units, with each element representing a layer\n",
    "        :type net_arch: list \n",
    "        :rtype: object\n",
    "        \"\"\"\n",
    "        self.activation_func = tanh  # The function used by each neuron to determine its output as a function of its inputs\n",
    "        self.activation_deriv = tanh_deriv  # Derivative of the activation function, used for gradient descent during backpropagation\n",
    "        self.layers = len(net_arch)  # number of layers\n",
    "        self.steps_per_epoch = 1000\n",
    "        self.net_arch = net_arch\n",
    "        \n",
    "        # Init weights to random values. This will be an array containing an array for each layer but the last (which \n",
    "        # doesn't connect to anything, strictly speaking, it just outputs the answer; each inner array contains\n",
    "        # one value per connection per neuron (with every layer including a bias unit of constant input 1, except for \n",
    "        # the last layer), i.e. if we have an architecture of [2, 2, 1], self.weights will be an array of length 2 containing\n",
    "        # a first array of length 6 (2 units connected with 2 units = 4 synapses + 1 bias connected with 2 units in first\n",
    "        # layer = 2 == 6) and a second array of length 3 (2 units connected with 1 unit = 2 + 1 bias connected with 1 unit\n",
    "        # in last layer = 1 == 3). \n",
    "        self.weights = []\n",
    "        for layer in range(len(net_arch) - 1):\n",
    "            w = 2 * np.random.rand(net_arch[layer] + 1, net_arch[layer + 1]) - 1\n",
    "            self.weights.append(w)\n",
    "            \n",
    "    def fit(self, data, labels, learning_rate: float = 0.1, epochs: int = 10, verbose: bool = False):\n",
    "        # bias units to the input layer. The input layer contains the input submitted to the net i.e. a pair of bools for\n",
    "        # our XOR() learning. We add the bias unit, which is just a leading 1.  \n",
    "        ones = np.ones((1, data.shape[0]))\n",
    "        Z = np.concatenate((ones.T, data), axis=1)\n",
    "        if verbose:\n",
    "            print(\"Z:\\n{}\".format(Z))\n",
    "        training = epochs * self.steps_per_epoch\n",
    "        if verbose:\n",
    "            print('total training steps: {}'.format(training))\n",
    "            print(\"data:\\n{}\\nshape:{}\".format(data, data.shape))\n",
    "            print(\"labels:\\n{}\\n\".format(labels))\n",
    "            print(\"bias unit(s) being added, data shape: {}\".format(data.shape[0]))\n",
    "        for k in range(training):\n",
    "            if k % self.steps_per_epoch == 0:\n",
    "                print('\\n\\nepochs: {}'.format(k / self.steps_per_epoch))\n",
    "                for s in data:\n",
    "                    print(s, nn.predict(s))\n",
    "            \n",
    "            # For the current iteration, select a random input pair to give to the net.\n",
    "            sample = np.random.randint(data.shape[0])\n",
    "            y = [Z[sample]]\n",
    "            if k % self.steps_per_epoch == 0 and verbose:\n",
    "                print(\"y before training of epoch {}:\\n{}\".format(k / self.steps_per_epoch, y))\n",
    "            \n",
    "            # Beginning with the input layer, calculate the activation for each unit. This means that we first iterate\n",
    "            # through the layers (except for the final, which does not need a bias unit input), we calculate the activation\n",
    "            # value for each neuron given the appropriate weight, we then use the activation function and \"transmit\" the \n",
    "            # information by storing it. This is obtained by computing the dot product between the weights and the units\n",
    "            # values, then applying the activation function to the resulting vector.\n",
    "            for i in range(len(self.weights) - 1):\n",
    "                activation = np.dot(y[i], self.weights[i])\n",
    "                if k % self.steps_per_epoch == 0 and verbose:\n",
    "                    print(self.weights[i].shape, y[i].shape)\n",
    "                activation_f = self.activation_func(activation)\n",
    "                activation_f = np.concatenate((np.ones(1), np.array(activation_f)))\n",
    "                y.append(activation_f)\n",
    "                if k % self.steps_per_epoch == 0 and verbose:\n",
    "                    print(\"values transmitted by layer {}:\\n{}\".format(i, activation_f))\n",
    "                \n",
    "            # Same thing for the final layer, just without adding a bias unit to it.\n",
    "            activation = np.dot(y[-1], self.weights[-1])\n",
    "            activation_f = self.activation_func(activation)\n",
    "            y.append(activation_f)\n",
    "            if k % self.steps_per_epoch == 0 and verbose:\n",
    "                print(\"y after last layer:\\n{}\".format(y))\n",
    "            \n",
    "            # We start from the last layer (output) and calculate the error of the net's response from the correct answer\n",
    "            error = labels[sample] - y[-1]\n",
    "            if k % self.steps_per_epoch == 0 and verbose:\n",
    "                print(\"net response error: {}\".format(error))\n",
    "            delta_vec = [error * self.activation_deriv(y[-1])]\n",
    "                         \n",
    "            # We go backwards towards the first layer, and descend the gradient by using the derivative of the activation\n",
    "            # function to compute the \"relative contribution\" of each weight in giving the final answer\n",
    "            for i in range(self.layers - 2, 0, -1):\n",
    "                if k % self.steps_per_epoch == 0 and verbose:\n",
    "                    print(i)\n",
    "                error = delta_vec[-1].dot(self.weights[i][1:].T)\n",
    "                error = error * self.activation_deriv(y[i][1:])\n",
    "                delta_vec.append(error)\n",
    "                \n",
    "            delta_vec.reverse()\n",
    "            if k % self.steps_per_epoch == 0 and verbose:\n",
    "                print(\"delta vector after epoch {}:\\n{}\".format(k / self.steps_per_epoch, delta_vec))\n",
    "                print(\"net answer: {}, error + answer = {}\".format(y[-1], labels[sample]))\n",
    "            \n",
    "            # We then go from first to last layer and update the weights given our gradient and our learning rate\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = y[i].reshape(1, nn.net_arch[i] + 1)\n",
    "                delta = delta_vec[i].reshape(1, nn.net_arch[i + 1])\n",
    "                if k % self.steps_per_epoch == 0 and verbose:\n",
    "                    print(\"layer: {}\".format(layer))\n",
    "                    print(\"delta for layer {}: {}\".format(i, delta))\n",
    "                    print(\"weights for layer {}:\\n{}\".format(i, self.weights[i]))\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "            \n",
    "            if k % self.steps_per_epoch == 0 and verbose:\n",
    "                print(\"new weights after epoch {}:\\n{}\".format(k / self.steps_per_epoch, self.weights))\n",
    "                \n",
    "    def predict(self, x):\n",
    "        val = np.concatenate((np.ones(1).T, np.array(x)))\n",
    "        for i in range(0, len(self.weights)):\n",
    "            val = self.activation_func(np.dot(val, self.weights[i]))\n",
    "            val = np.concatenate((np.ones(1).T, np.array(val)))\n",
    "            \n",
    "        return val[1]\n",
    "        \n",
    "    def plot_decision_regions(self, X, y, points=200):\n",
    "        markers = ('o', '^')\n",
    "        colors = ('red', 'blue')\n",
    "        cmap = ListedColormap(colors)\n",
    "        \n",
    "        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        \n",
    "        resolution = max(x1_max - x1_min, x2_max - x2_min) / float(points)\n",
    "        \n",
    "        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "        my_input = np.array([xx1.ravel(), xx2.ravel()]).T\n",
    "        Z = np.empty(0)\n",
    "        for i in range(my_input.shape[0]):\n",
    "            val = nn.predict(np.array(my_input[i]))\n",
    "            if val < 0.5:\n",
    "                val = 0\n",
    "            if val >= 0.5:\n",
    "                val = 1\n",
    "            Z = np.append(Z, val)\n",
    "        Z = Z.reshape(xx1.shape)\n",
    "        \n",
    "        plt.pcolormesh(xx1, xx2, Z, cmap=cmap)\n",
    "        plt.xlim(xx1.min(), xx1.max())\n",
    "        plt.ylim(xx2.min(), xx2.max())\n",
    "        \n",
    "        classes = [\"False\", \"True\"]\n",
    "        \n",
    "        for idx, cl in enumerate(np.unique(y)):\n",
    "            plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=1.0, c=colors[idx], edgecolors='black', \n",
    "                        marker=markers[idx], s=80, label=classes[idx])\n",
    "        plt.xlabel('x-axis')\n",
    "        plt.ylabel('y-axis')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example 1, verbose\n",
    "np.random.seed(1)\n",
    "nn = NeuralNetwork([2, 2, 1])\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "nn.fit(X, y, epochs=10, verbose=True)\n",
    "\n",
    "print(\"final prediction\")\n",
    "for s in X:\n",
    "    print(s, nn.predict(s))\n",
    "    \n",
    "nn.plot_decision_regions(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example 2, larger hidden neuron to show how the decision boundaries change\n",
    "np.random.seed(1)\n",
    "nn = NeuralNetwork([2, 6, 1])\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "nn.fit(X, y, epochs=10, verbose=False)\n",
    "\n",
    "print(\"final prediction\")\n",
    "for s in X:\n",
    "    print(s, nn.predict(s))\n",
    "    \n",
    "nn.plot_decision_regions(X, y)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}