{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Python Deep Learning\n",
    "# Ivan Vasilev, Daniel Slater, Gianmario Spacagna, Peter Roelants, Valentino Zocca\n",
    "## Second Edition - Packt \n",
    "# Notes / Experiments with Python (and PyTorch, TensorFlow and Keras, among others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Imports and plot settings\n",
    "PLT_TYPE = 'inline'  # restart Jupyter kernel before switching back to 'inline'\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "if PLT_TYPE.lower() == 'widget':\n",
    "    %matplotlib widget\n",
    "elif PLT_TYPE.lower() == 'inline':\n",
    "    pass\n",
    "elif PLT_TYPE.lower() == 'window':\n",
    "    matplotlib.use('Qt5Agg')\n",
    "    plt.ion()\n",
    "else:\n",
    "    raise SyntaxError(\"PLT_TYPE can either be 'inline', 'widget' or 'window\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chapter 1. Machine Learning - an Introduction\n",
    "* AI can be defined, among other things, as a system that interact with its environment, using sensors and actuators.\n",
    "* Machine learning can be seen as the way an AI tries and formulate appropriate answers as a function of the data available.\n",
    "* Deep learning is a subfield of machine learning, mainly defined by the use of certain techniques/approaches. The most \n",
    "representative class of these methods are deep neural networks.\n",
    "\n",
    "### Approaches to learning\n",
    "* Supervised learning\n",
    "* Unsupervised learning\n",
    "* Reinforcement learning\n",
    "\n",
    "### Supervised Learning\n",
    "Supervised learning involves the use of previously-labeled data in order to learn its features, so they can then classify\n",
    "new, similar but unlabeled data. This approaches needs **training data**. \n",
    "\n",
    "One way of thinking of supervised learning is as a function _f_ defined over a dataset, which comprises information organized\n",
    "by **features**. \n",
    "\n",
    "```f: space of features -> classes = (discrete values or real values)``` \n",
    "\n",
    "We will use the ```MNIST``` dataset (Modified National Institute of Standard and Technology) to work on handwritten digits\n",
    "recognition by means of either classification or regression approaches. In this case, we'll use images of 28x28 pixels of\n",
    "size. **Our algorithm will use a 28x28 = 784-dimensional feature space to classify the digits**.\n",
    "\n",
    "#### Linear and Logistic Regression\n",
    "Regression algorithms are a type of supervised algos that uses features of the input data to predict a value, such as the\n",
    "cost of a certain type of good, given a feature set. Regression tries to find the value of the parameters for the function\n",
    "that best fits an input dataset.\n",
    "\n",
    "In linear regression algos, the goal is to minimize the cost function (error of prediction from truth) by finding appropriate\n",
    "values for the parameters of the function, over the input data that best approximates the target value. A popular example\n",
    "is **mean square error** (MSE). A pseudo-code representation of this would be:\n",
    "```\n",
    "Given:\n",
    "* a dataset comprising input/target pairs as (x[i], t[i]) for every i in range(len(dataset))\n",
    "* a vector w containing random values of len(w) == len(features) == len(x[i])\n",
    "For an arbitrarily large number of times, repeat:\n",
    "    Err = 0  # initialize cost\n",
    "    for every i in range(len(dataset))\n",
    "        Err += (x[i] * w - t[i]) ** 2\n",
    "    MSE = Err / len(dataset)\n",
    "```\n",
    "\n",
    "We iterate over the training data to compute the cost function to obtain the MSE, and then we use the gradient-descent\n",
    "algorithm to update _w_ accordingly. This involves computing the derivatives of the cost function with respect of each\n",
    "weight, in order to determine how the cost changes with respect of each weight. We'll see how this is remarkably similar\n",
    "to the process used to train neural networks.\n",
    "\n",
    "We can adapt the regression logic to a situation in which we want to reduce the outcome of our algorithm to a discrete,\n",
    "categorical output (as opposed to a real-value output, such as the cost of a given good above). In this case we can use\n",
    "**logistic regression**. It is imaginable as a probability between two certain possible outcomes, and the response label\n",
    "is either one or the other possible outcome. To make this technique usable for classification problems, we need to introduce\n",
    "a rule that determines the class based on the logistic function output (i.e. similar to a threshold). Boh.\n",
    "\n",
    "#### Support Vector Machines\n",
    "A support vector machine is a type of supervised machine learning algo used mainly for classification. It belongs\n",
    "to the kernel method class of algos. An SVM tries to find a hyperplane separating the sample across its features.\n",
    "\n",
    "#### Decision Trees\n",
    "A decision tree takes on classification problems by representing the whole computation and decision process as a tree.\n",
    "It is composed of decision nodes, in charge of testing specific attributes of the data, and leaf nodes, indicating the \n",
    "value of the target attribute. To begin a classification, we start at the root and navigate down the nodes until we \n",
    "reach a leaf. The _Iris flower dataset_ can be used to show this algorithm. We can create a decision tree to decide which\n",
    "species a certain flower belongs to:\n",
    "```\n",
    "          Petal Length < 2.5\n",
    "                  |\n",
    "    *Iris Setosa* - Petal Width < 1.8\n",
    "                           |                 \n",
    "        Petal Length < 4.9 - *Iris Virginica*\n",
    "                  |\n",
    "*Iris Versicolor* - *Iris Virginica*\n",
    "```\n",
    "#### Naive Bayes\n",
    "No, I'll come here later I think.\n",
    "\n",
    "### Unsupervised Learning\n",
    "This class involves methods that try to come to its own conclusions about the data without labels/ground truth. With\n",
    " **cluster analysis**, we try finding \"natural\" clustering behaviors in the data, given certain features, to derive\n",
    "the different classes possibly underlying the data (_k-means_ are an example of this subclass).\n",
    "\n",
    "A different approach, **recurrent neural networks**, make use of the \"context\" of data (i.e., in natural language processing,\n",
    "each word in a phrase is submitted, together with its neighboring words - the context - to simple neural nets). \n",
    "\n",
    "With **generative adversarial networks (GANs)**, we first train a network with large dataset, and then we use the network\n",
    "to produce new examples similar to the training dataset. They can be used to colorize black and white photographs, alter\n",
    "facial expressions in images and more.\n",
    "\n",
    "### Reinforcement Learning\n",
    "This third class involves having an algorithm try to maximize certain rewards obtained by interacting with an environment.\n",
    "The _agent_ takes an action that changes the state of the environment. It then uses the new state and the reward to \n",
    "determine its next action. It has to do with using previous, progressively accumulated experience to improve in the task,\n",
    "as opposed to just using ground truths to derive rules. \n",
    "\n",
    "#### Q-learning\n",
    "Q-learning is an off-policy temporal-difference reinforcement learning algorithm. A suitable example is imagining trying\n",
    "to create an ML agent that plays and tries to win a chess game. For any given time, the state of the chess game\n",
    "is represented by the board configuration (i.e. the location of the pieces on the board). The agent needs then to take an\n",
    "action, a, by moving a piece, thus changing the state of the board. The problem may be represented as a graph. Each\n",
    "vertex represents a given board configuration, while each edge is a move that brings the state from one configuration of\n",
    "the pieces (node) to another. By making a move, the agent moves from one edge to another, and uses a _Q-table_ to decide\n",
    "which move to take. A Q-table has one row for each different board configuration (state, ```s```), and a column for each\n",
    "possible action that the agent can take (a move, ```a```). A given cell of the Q-table, ```q(s, a)```, contains the \n",
    "potential total reward obtained for the remainder of the game if the agent takes an action ```a``` from the current state\n",
    "```s``` and it's called **Q-value**. The Q-table is first initialized with arbitrary values, and it's then filled as the\n",
    "game progresses and finishes. The Q-values are used to determine the \"attractiveness\" of a certain move in a certain state\n",
    "in the attempt to ultimately win the game. It allows the agent to seek high potential rewards by exploring.\n",
    "\n",
    "### Components of an ML solution\n",
    "* Learner\n",
    "* Training data\n",
    "* Representation (how we express data in terms of selected features to provide the learner with)\n",
    "* Goal (the reason to learn from the data, the aim)\n",
    "* Target (what is being learned as well as the final output)\n",
    "\n",
    "#### Creation of the test case\n",
    "* Training set (what we use in training phase)\n",
    "* Validation set (to evaluate the accuracy of the algo using unknown data. Sometimes we can fine-tune the model after\n",
    "feedback from using the validation set, and it is used to determine when to stop learning)\n",
    "* Test set (to use only **once** after training, to prevent introducing bias when over-tuning the data in successive\n",
    "attempts with the same data points)\n",
    "\n",
    "...\n",
    "\n",
    "### Introduction to PyTorch\n",
    "Finally!!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "150\n",
      "Epoch 1 Loss: 1.2181\n",
      "Epoch 10 Loss: 0.6745\n",
      "Epoch 20 Loss: 0.2447\n",
      "Epoch 30 Loss: 0.1397\n",
      "Epoch 40 Loss: 0.1001\n",
      "Epoch 50 Loss: 0.0855\n",
      "Errors: 0, Accuracy: 100%\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Download and parse a CSV file from the Internet, containing 150 rows of the IRIS flower dataset\n",
    "my_dataset = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \n",
    "                         names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "# Recode the categorical column 'species' from strings to codes ([0, 1, 2]) for the 3 flowers\n",
    "my_dataset['species'] = pd.Categorical(my_dataset['species']).codes\n",
    "# Shuffle the order of the rows of the dataset\n",
    "my_dataset = my_dataset.sample(frac=1, random_state=1234)\n",
    "\n",
    "print(len(my_dataset.values))\n",
    "train_input = my_dataset.values[:120, :4]  # The majority of the rows will be training dataset, exclude the column containing the ground truth\n",
    "train_target = my_dataset.values[:120, 4]  # Store the ground truth only for 120 of the rows, like above\n",
    "\n",
    "test_input = my_dataset.values[120:, :4]  # Same for the test dataset, only using the last 30 rows for this\n",
    "test_target = my_dataset.values[120:, 4]\n",
    "\n",
    "torch.manual_seed(1234)  # reproducibility\n",
    "\n",
    "hidden_units = 5  # One hidden layer containing 5 neurons\n",
    "\n",
    "# Feed-forward network, 1 hidden layer (5 units), rectified linear activation function and 1 output layer with 3 units\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, hidden_units),  # first layer, 4 neurons, one per variable of interest (sepal_length, sepal_width, etc.)\n",
    "    torch.nn.ReLU(),  # rectified linear activation function\n",
    "    torch.nn.Linear(hidden_units, 3)  # output layer, 3 units (one per possible flower type)\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "my_crit = torch.nn.CrossEntropyLoss()  # cross entropy loss as loss function\n",
    "my_optim = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)  # stochastic gradient descent as optimizer\n",
    "\n",
    "# training loop\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # noinspection PyArgumentList\n",
    "    inputs = torch.autograd.Variable(torch.Tensor(train_input).float())\n",
    "    # noinspection PyArgumentList\n",
    "    targets = torch.autograd.Variable(torch.Tensor(train_target).long())\n",
    "    \n",
    "    my_optim.zero_grad()  # Start with fresh zeroed gradients for the current epoch\n",
    "    out = net(inputs)  # submit the inputs to the net\n",
    "    loss = my_crit(out, targets)  # how far were we from the truth?\n",
    "    loss.backward()  # Compute contributions of each weight\n",
    "    my_optim.step()  # attempt improvement\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print('Epoch %d Loss: %.4f' % (epoch + 1, loss.item()))\n",
    "        \n",
    "# noinspection PyArgumentList\n",
    "inputs = torch.autograd.Variable(torch.Tensor(test_input).float())\n",
    "# noinspection PyArgumentList\n",
    "targets = torch.autograd.Variable(torch.Tensor(test_target).long())\n",
    "\n",
    "my_optim.zero_grad()\n",
    "out = net(inputs)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "error_count = test_target.size - np.count_nonzero((targets == predicted).numpy())\n",
    "# noinspection PyTypeChecker\n",
    "print('Errors: %d, Accuracy: %d%%' % (error_count, 100 * torch.sum(targets == predicted) / test_target.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It worked perfectly.\n",
    "## 2. Neural Networks\n",
    "### Intro to neural networks\n",
    "* Information occurs mathematically, over simple elements called **neurons**\n",
    "* Neurons are connected and exchange signals (i.e. numbers) between each other through links\n",
    "* Each link has a **weight**, determining how information is processed as it passes through\n",
    "* Each neuron has an internal state, determined by all the incoming links\n",
    "* Each neuron has an **activation function** which determines the output signal as a function of its state\n",
    "\n",
    "The **architecture** of a neural network defines what type of connectivity the units have (i.e. feedforward, recurrent, multi-\n",
    "or single-layered, etc.), the number of layers and of neurons within each layer.\n",
    "The **learning** describes how the network adapt and improves; common tools are _gradient descent_ and _backpropagation_.\n",
    "\n",
    "### Intro to neurons\n",
    "Mathematically, a neuron is defined as:\n",
    "```\n",
    "y = f(sum(x[:] * w[:]) + b)\n",
    "```\n",
    "where ```x``` represents the inputs, and ```w``` represents the weights. ```b``` represents the _bias_ and its input is \n",
    "always 1.\n",
    "\n",
    "### Intro to layers\n",
    "In a neural network, the input layer represents the dataset and the initial conditions: if the net deals with grayscale \n",
    "images, the units in the first layer will represent the pixel intensity. The output layer can have more than one neuron:\n",
    "usually we find one unit per possible answer/class.\n",
    "\n",
    "### Multi-layer networks\n",
    "Single layer networks can only classify linearly separable classes, but by adding **hidden layers** we can surpass this\n",
    "limitation. Another condition for multi-layer network to classify linearly-inseparable classes is that their **activation\n",
    "function must not be linear**.\n",
    "\n",
    "### Different types of activation function\n",
    "The most common activation functions are the following:\n",
    "* ```f(a) = a``` - **identity function**\n",
    "* ```f(a) = [1 if a >= 0, otherwise 0]``` - **threshold activity function**\n",
    "* ```f(a) = 1 / (1 + exp(-a))``` - **logistic function**, or **logistic sigmoid** (0, 1)\n",
    "* ```f(a) = 2 / (1 + exp(-a)) = (1 - exp(-a)) / (1 + exp(-a))``` - **bipolar sigmoid** (-1, 1)\n",
    "* ```f(a) = (exp(a)-exp(-a)) / (exp(a) + exp(-a)) = (1 - exp(-2 * a)) / (1 + exp(-2 * a))``` - **hyperbolic tangent**, tanh (-1, 1)\n",
    "* ``` f(a) = [a if a >= 0, otherwise 0]``` - **rectified linear unit**, **ReLU** (0, Inf)\n",
    "\n",
    "### Example\n",
    "Let's build a very simple net, one hidden layer with two neurons, and single input and output neurons. It will support\n",
    "the idea that under the _Universal Approximation Theorem_ any continuous function on compact subsets of Rn can be \n",
    "approximated by a neural network with at least one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The step function starts at -5.0 and ends at 5.0\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\Luca\\Anaconda3\\envs\\py37ML\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Luca\\Anaconda3\\envs\\py37ML\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight_value = 1000\n",
    "\n",
    "bias_value_1 = 5000\n",
    "\n",
    "bias_value_2 = -5000\n",
    "\n",
    "plt.axis([-10, 10, -1, 10])\n",
    "\n",
    "print(\"The step function starts at {0} and ends at {1}\".format(-bias_value_1 / weight_value, -bias_value_2 / weight_value))\n",
    "\n",
    "inputs = np.arange(-10, 10, 0.01)\n",
    "outputs = list()\n",
    "\n",
    "for x in inputs:\n",
    "    y1 = 1.0 / (1.0 + np.exp(-weight_value * x - bias_value_1))\n",
    "    y2 = 1.0 / (1.0 + np.exp(-weight_value * x - bias_value_2))\n",
    "    \n",
    "    w = 7\n",
    "    \n",
    "    y = y1 * w - y2 * w\n",
    "    \n",
    "    outputs.append(y)\n",
    "plt.plot(inputs, outputs, lw=2, color='black')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Pyplot backends and settings\n",
    "Importing ```matplotlib``` and calling the method ```matplotlib.use()``` allows to change pyplot backend.\n",
    "* **default** DON'T KNOW \n",
    "* **'WebAgg'** opens a new browser tab, interactive\n",
    "* **'Qt5Agg'** new window, interactive (use ```plt.show(block=False)```!!!)\n",
    "\n",
    "Beware: after setting the backend to ```Qt5Agg``` (or any interactive one), to have inline plots in the notebook again\n",
    "the Jupyter kernel must be restarted before disabling the ```matplotlib.use()``` call.\n",
    "\n",
    "### Training neural networks\n",
    "We will now explore how the training procedure works on a very simple, 1-layer network using an algorithm called\n",
    "**gradient descent**, then extending the example to a deep forward network using **backpropagation**. During training, we\n",
    "use these tools with the aim of decreasing the network prediction error as much as possible. It is a minima finding problem.\n",
    "\n",
    "### Linear regression\n",
    "In vector notation, the output of a linear regression is a single value, ```y```, and is equal to the dot product of the\n",
    "input values ```x``` and the weights ```w```. Linear regression **is a special case of a neural network** with a single\n",
    "unit and identity activation function. \n",
    "\n",
    "Gradient descent works as follows:\n",
    "1. Initialize the weights ```w``` with random values\n",
    "2. Repeat:\n",
    "    * Compute the MSE for all the samples of the training set (```(sum(x * w - t) ^ 2) / n```)\n",
    "    * Compute the derivative of the MSE for each weight, update the weights to minimize it, until MSE is below threshold\n",
    "    \n",
    "The learning rate defines the ratio by which the weight adjusts as new data arrives. \n",
    "\n",
    "### Logistic regression\n",
    "Same as above, but with a different activation function (logistic sigmoid activation).\n",
    "\n",
    "### Backpropagation\n",
    "For a 1-layer network, updating the weights using gradient descent is straightforward since we can compare the output\n",
    "with the target and update the weights. In multi-layer networks, we could only do this for the weights that connect\n",
    "the the final hidden layer to the output layer, because we don't have target values for the intermediate representations.\n",
    "\n",
    "In this case, what we do is calculate the error in the final hidden layer and estimate what it would have been in the \n",
    "previous layer. Then, we propagate the error back from the last layer to the first.\n",
    "\n",
    "### Code example: net for the XOR function\n",
    "XOR is a linearly inseparable function, so we will use it to show a net with a hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# noinspection PyShadowingNames\n",
    "def tanh(x: float):\n",
    "    return (1.0 - np.exp(-2 * x)) / (1.0 + np.exp(-2 * x))\n",
    "\n",
    "# noinspection PyShadowingNames\n",
    "def tanh_deriv(x: float):\n",
    "    return (1 + tanh(x)) * (1 - tanh(x))\n",
    "\n",
    "# noinspection PyPep8Naming,PyShadowingNames,PyMethodMayBeStatic\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    NeuralNetwork represents a simple feedforward network, consisting of an architecture represented by an array whose \n",
    "    length defines the number of layers and the elements the number of units in each layer. Implements methods to fit the\n",
    "    data, to make a prediction and to plot the results.\n",
    "    \"\"\"\n",
    "    def __init__(self, net_arch: list):\n",
    "        \"\"\"\n",
    "        :param net_arch: Architecture of the net, as an array of number of units, with each element representing a layer\n",
    "        :type net_arch: list \n",
    "        :rtype: object\n",
    "        \"\"\"\n",
    "        self.activation_func = tanh  # The function used by each neuron to determine its output as a function of its inputs\n",
    "        self.activation_deriv = tanh_deriv  # Derivative of the activation function, used for gradient descent during backpropagation\n",
    "        self.layers = len(net_arch)  # number of layers\n",
    "        self.steps_per_epoch = 1000\n",
    "        self.net_arch = net_arch\n",
    "        \n",
    "        # Init weights to random values. This will be an array containing an array for each layer but the last (which \n",
    "        # doesn't connect to anything, strictly speaking, it just outputs the answer; each inner array contains\n",
    "        # one value per connection per neuron (with every layer including a bias unit of constant input 1, except for \n",
    "        # the last layer), i.e. if we have an architecture of [2, 2, 1], self.weights will be an array of length 2 containing\n",
    "        # a first array of length 6 (2 units connected with 2 units = 4 synapses + 1 bias connected with 2 units in first\n",
    "        # layer = 2 == 6) and a second array of length 3 (2 units connected with 1 unit = 2 + 1 bias connected with 1 unit\n",
    "        # in last layer = 1 == 3). \n",
    "        self.weights = []\n",
    "        for layer in range(len(net_arch) - 1):\n",
    "            w = 2 * np.random.rand(net_arch[layer] + 1, net_arch[layer + 1]) - 1\n",
    "            self.weights.append(w)\n",
    "            \n",
    "    def fit(self, data, labels, learning_rate: float = 0.1, epochs: int = 10, verbose: bool = False):\n",
    "        # bias units to the input layer. The input layer contains the input submitted to the net i.e. a pair of bools for\n",
    "        # our XOR() learning. We add the bias unit, which is just a leading 1.  \n",
    "        ones = np.ones((1, data.shape[0]))\n",
    "        Z = np.concatenate((ones.T, data), axis=1)\n",
    "        if verbose:\n",
    "            print(\"Z:\\n{}\".format(Z))\n",
    "        training = epochs * self.steps_per_epoch\n",
    "        if verbose:\n",
    "            print('total training steps: {}'.format(training))\n",
    "            print(\"data:\\n{}\\nshape:{}\".format(data, data.shape))\n",
    "            print(\"labels:\\n{}\\n\".format(labels))\n",
    "            print(\"bias unit(s) being added, data shape: {}\".format(data.shape[0]))\n",
    "        for k in range(training):\n",
    "            if k % self.steps_per_epoch == 0:\n",
    "                print('\\n\\nepochs: {}'.format(k / self.steps_per_epoch))\n",
    "                for s in data:\n",
    "                    print(s, nn.predict(s))\n",
    "            \n",
    "            # For the current iteration, select a random input pair to give to the net.\n",
    "            sample = np.random.randint(data.shape[0])\n",
    "            y = [Z[sample]]\n",
    "            if k % self.steps_per_epoch == 0 and verbose:\n",
    "                print(\"y before training of epoch {}:\\n{}\".format(k / self.steps_per_epoch, y))\n",
    "            \n",
    "            # Beginning with the input layer, calculate the activation for each unit. This means that we first iterate\n",
    "            # through the layers (except for the final, which does not need a bias unit input), we calculate the activation\n",
    "            # value for each neuron given the appropriate weight, we then use the activation function and \"transmit\" the \n",
    "            # information by storing it. This is obtained by computing the dot product between the weights and the units\n",
    "            # values, then applying the activation function to the resulting vector.\n",
    "            for i in range(len(self.weights) - 1):\n",
    "                activation = np.dot(y[i], self.weights[i])\n",
    "                if k % self.steps_per_epoch == 0 and verbose:\n",
    "                    print(self.weights[i].shape, y[i].shape)\n",
    "                activation_f = self.activation_func(activation)\n",
    "                activation_f = np.concatenate((np.ones(1), np.array(activation_f)))\n",
    "                y.append(activation_f)\n",
    "                if k % self.steps_per_epoch == 0 and verbose:\n",
    "                    print(\"values transmitted by layer {}:\\n{}\".format(i, activation_f))\n",
    "                \n",
    "            # Same thing for the final layer, just without adding a bias unit to it.\n",
    "            activation = np.dot(y[-1], self.weights[-1])\n",
    "            activation_f = self.activation_func(activation)\n",
    "            y.append(activation_f)\n",
    "            if k % self.steps_per_epoch == 0 and verbose:\n",
    "                print(\"y after last layer:\\n{}\".format(y))\n",
    "            \n",
    "            # We start from the last layer (output) and calculate the error of the net's response from the correct answer\n",
    "            error = labels[sample] - y[-1]\n",
    "            if k % self.steps_per_epoch == 0 and verbose:\n",
    "                print(\"net response error: {}\".format(error))\n",
    "            delta_vec = [error * self.activation_deriv(y[-1])]\n",
    "                         \n",
    "            # We go backwards towards the first layer, and descend the gradient by using the derivative of the activation\n",
    "            # function to compute the \"relative contribution\" of each weight in giving the final answer\n",
    "            for i in range(self.layers - 2, 0, -1):\n",
    "                if k % self.steps_per_epoch == 0 and verbose:\n",
    "                    print(i)\n",
    "                error = delta_vec[-1].dot(self.weights[i][1:].T)\n",
    "                error = error * self.activation_deriv(y[i][1:])\n",
    "                delta_vec.append(error)\n",
    "                \n",
    "            delta_vec.reverse()\n",
    "            if k % self.steps_per_epoch == 0 and verbose:\n",
    "                print(\"delta vector after epoch {}:\\n{}\".format(k / self.steps_per_epoch, delta_vec))\n",
    "                print(\"net answer: {}, error + answer = {}\".format(y[-1], labels[sample]))\n",
    "            \n",
    "            # We then go from first to last layer and update the weights given our gradient and our learning rate\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = y[i].reshape(1, nn.net_arch[i] + 1)\n",
    "                delta = delta_vec[i].reshape(1, nn.net_arch[i + 1])\n",
    "                if k % self.steps_per_epoch == 0 and verbose:\n",
    "                    print(\"layer: {}\".format(layer))\n",
    "                    print(\"delta for layer {}: {}\".format(i, delta))\n",
    "                    print(\"weights for layer {}:\\n{}\".format(i, self.weights[i]))\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "            \n",
    "            if k % self.steps_per_epoch == 0 and verbose:\n",
    "                print(\"new weights after epoch {}:\\n{}\".format(k / self.steps_per_epoch, self.weights))\n",
    "                \n",
    "    def predict(self, x):\n",
    "        val = np.concatenate((np.ones(1).T, np.array(x)))\n",
    "        for i in range(0, len(self.weights)):\n",
    "            val = self.activation_func(np.dot(val, self.weights[i]))\n",
    "            val = np.concatenate((np.ones(1).T, np.array(val)))\n",
    "            \n",
    "        return val[1]\n",
    "        \n",
    "    def plot_decision_regions(self, X, y, points=200):\n",
    "        markers = ('o', '^')\n",
    "        colors = ('red', 'blue')\n",
    "        cmap = ListedColormap(colors)\n",
    "        \n",
    "        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        \n",
    "        resolution = max(x1_max - x1_min, x2_max - x2_min) / float(points)\n",
    "        \n",
    "        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "        my_input = np.array([xx1.ravel(), xx2.ravel()]).T\n",
    "        Z = np.empty(0)\n",
    "        for i in range(my_input.shape[0]):\n",
    "            val = nn.predict(np.array(my_input[i]))\n",
    "            if val < 0.5:\n",
    "                val = 0\n",
    "            if val >= 0.5:\n",
    "                val = 1\n",
    "            Z = np.append(Z, val)\n",
    "        Z = Z.reshape(xx1.shape)\n",
    "        \n",
    "        plt.pcolormesh(xx1, xx2, Z, cmap=cmap)\n",
    "        plt.xlim(xx1.min(), xx1.max())\n",
    "        plt.ylim(xx2.min(), xx2.max())\n",
    "        \n",
    "        classes = [\"False\", \"True\"]\n",
    "        \n",
    "        for idx, cl in enumerate(np.unique(y)):\n",
    "            plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=1.0, c=colors[idx], edgecolors='black', \n",
    "                        marker=markers[idx], s=80, label=classes[idx])\n",
    "        plt.xlabel('x-axis')\n",
    "        plt.ylabel('y-axis')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Z:\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]]\n",
      "total training steps: 10000\n",
      "data:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "shape:(4, 2)\n",
      "labels:\n",
      "[0 1 1 0]\n",
      "\n",
      "bias unit(s) being added, data shape: 4\n",
      "\n",
      "\n",
      "epochs: 0.0\n",
      "[0 0] -0.5798249476496324\n",
      "[0 1] -0.3243739160609326\n",
      "[1 0] -0.365009704539086\n",
      "[1 1] -0.19670589979422728\n",
      "y before training of epoch 0.0:\n",
      "[array([1., 1., 0.])]\n",
      "(3, 2) (3,)\n",
      "values transmitted by layer 0:\n",
      "[ 1.         -0.8228976   0.04528314]\n",
      "y after last layer:\n",
      "[array([1., 1., 0.]), array([ 1.        , -0.8228976 ,  0.04528314]), array([-0.3650097])]\n",
      "net response error: [1.3650097]\n",
      "1\n",
      "delta vector after epoch 0.0:\n",
      "[array([-0.20064317, -0.24687155]), array([1.19815981])]\n",
      "net answer: [-0.3650097], error + answer = 1\n",
      "layer: [[1. 1. 0.]]\n",
      "delta for layer 0: [[-0.20064317 -0.24687155]]\n",
      "weights for layer 0:\n",
      "[[-0.16595599  0.44064899]\n",
      " [-0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281]]\n",
      "layer: [[ 1.         -0.8228976   0.04528314]]\n",
      "delta for layer 1: [[1.19815981]]\n",
      "weights for layer 1:\n",
      "[[-0.62747958]\n",
      " [-0.30887855]\n",
      " [-0.20646505]]\n",
      "new weights after epoch 0.0:\n",
      "[array([[-0.18602031,  0.41596183],\n",
      "       [-1.01983557, -0.42002201],\n",
      "       [-0.70648822, -0.81532281]]), array([[-0.5076636 ],\n",
      "       [-0.40747483],\n",
      "       [-0.20103941]])]\n",
      "\n",
      "\n",
      "epochs: 1.0\n",
      "[0 0] 0.1030108432552569\n",
      "[0 1] 0.6994995764459226\n",
      "[1 0] 0.7334092456937719\n",
      "[1 1] 0.0686804567135108\n",
      "y before training of epoch 1.0:\n",
      "[array([1., 0., 1.])]\n",
      "(3, 2) (3,)\n",
      "values transmitted by layer 0:\n",
      "[ 1.         -0.63510296  0.30375525]\n",
      "y after last layer:\n",
      "[array([1., 0., 1.]), array([ 1.        , -0.63510296,  0.30375525]), array([0.69949958])]\n",
      "net response error: [0.30050042]\n",
      "1\n",
      "delta vector after epoch 1.0:\n",
      "[array([-0.21070194,  0.25857479]), array([0.19085489])]\n",
      "net answer: [0.69949958], error + answer = 1\n",
      "layer: [[1. 0. 1.]]\n",
      "delta for layer 0: [[-0.21070194  0.25857479]]\n",
      "weights for layer 0:\n",
      "[[ 0.38443797  1.36752688]\n",
      " [-1.09671645 -0.97241651]\n",
      " [-1.13436088 -1.05387549]]\n",
      "layer: [[ 1.         -0.63510296  0.30375525]]\n",
      "delta for layer 1: [[0.19085489]]\n",
      "weights for layer 1:\n",
      "[[-0.60845734]\n",
      " [-1.61247702]\n",
      " [ 1.48372215]]\n",
      "new weights after epoch 1.0:\n",
      "[array([[ 0.36336778,  1.39338436],\n",
      "       [-1.09671645, -0.97241651],\n",
      "       [-1.15543107, -1.02801801]]), array([[-0.58937185],\n",
      "       [-1.62459827],\n",
      "       [ 1.48951947]])]\n",
      "\n",
      "\n",
      "epochs: 2.0\n",
      "[0 0] 0.011281201488060158\n",
      "[0 1] 0.9494804233776619\n",
      "[1 0] 0.9609754991085776\n",
      "[1 1] 0.01243435226347843\n",
      "y before training of epoch 2.0:\n",
      "[array([1., 1., 1.])]\n",
      "(3, 2) (3,)\n",
      "values transmitted by layer 0:\n",
      "[ 1.         -0.97926885 -0.59376175]\n",
      "y after last layer:\n",
      "[array([1., 1., 1.]), array([ 1.        , -0.97926885, -0.59376175]), array([0.01243435])]\n",
      "net response error: [-0.01243435]\n",
      "1\n",
      "delta vector after epoch 2.0:\n",
      "[array([ 0.01263531, -0.02015199]), array([-0.01243243])]\n",
      "net answer: [0.01243435], error + answer = 0\n",
      "layer: [[1. 1. 1.]]\n",
      "delta for layer 0: [[ 0.01263531 -0.02015199]]\n",
      "weights for layer 0:\n",
      "[[ 0.56882255  1.8731301 ]\n",
      " [-1.43693532 -1.24714449]\n",
      " [-1.41130995 -1.3094419 ]]\n",
      "layer: [[ 1.         -0.97926885 -0.59376175]]\n",
      "delta for layer 1: [[-0.01243243]]\n",
      "weights for layer 1:\n",
      "[[-0.94055746]\n",
      " [-2.34516012]\n",
      " [ 2.26277592]]\n",
      "new weights after epoch 2.0:\n",
      "[array([[ 0.57008608,  1.8711149 ],\n",
      "       [-1.43567179, -1.24915969],\n",
      "       [-1.41004641, -1.3114571 ]]), array([[-0.9418007 ],\n",
      "       [-2.34394265],\n",
      "       [ 2.26351411]])]\n",
      "\n",
      "\n",
      "epochs: 3.0\n",
      "[0 0] 0.004070520743134206\n",
      "[0 1] 0.9791988284439718\n",
      "[1 0] 0.9811940681055433\n",
      "[1 1] 0.007122463734488729\n",
      "y before training of epoch 3.0:\n",
      "[array([1., 0., 1.])]\n",
      "(3, 2) (3,)\n",
      "values transmitted by layer 0:\n",
      "[ 1.         -0.71821582  0.56282078]\n",
      "y after last layer:\n",
      "[array([1., 0., 1.]), array([ 1.        , -0.71821582,  0.56282078]), array([0.97919883])]\n",
      "net response error: [0.02080117]\n",
      "1\n",
      "delta vector after epoch 3.0:\n",
      "[array([-0.01457123,  0.01684635]), array([0.00901553])]\n",
      "net answer: [0.97919883], error + answer = 1\n",
      "layer: [[1. 0. 1.]]\n",
      "delta for layer 0: [[-0.01457123  0.01684635]]\n",
      "weights for layer 0:\n",
      "[[ 0.61122692  2.01029336]\n",
      " [-1.51981632 -1.34681328]\n",
      " [-1.51517705 -1.37334114]]\n",
      "layer: [[ 1.         -0.71821582  0.56282078]]\n",
      "delta for layer 1: [[0.00901553]]\n",
      "weights for layer 1:\n",
      "[[-1.01369551]\n",
      " [-2.60353092]\n",
      " [ 2.52570178]]\n",
      "new weights after epoch 3.0:\n",
      "[array([[ 0.60976979,  2.011978  ],\n",
      "       [-1.51981632, -1.34681328],\n",
      "       [-1.51663418, -1.37165651]]), array([[-1.01279396],\n",
      "       [-2.60417843],\n",
      "       [ 2.52620919]])]\n",
      "\n",
      "\n",
      "epochs: 4.0\n",
      "[0 0] 0.0009609702849334557\n",
      "[0 1] 0.9878813694025445\n",
      "[1 0] 0.986923308255853\n",
      "[1 1] 0.003555706413675254\n",
      "y before training of epoch 4.0:\n",
      "[array([1., 1., 1.])]\n",
      "(3, 2) (3,)\n",
      "values transmitted by layer 0:\n",
      "[ 1.         -0.98651188 -0.62015219]\n",
      "y after last layer:\n",
      "[array([1., 1., 1.]), array([ 1.        , -0.98651188, -0.62015219]), array([0.00355571])]\n",
      "net response error: [-0.00355571]\n",
      "1\n",
      "delta vector after epoch 4.0:\n",
      "[array([ 0.00417996, -0.00660217]), array([-0.00355566])]\n",
      "net answer: [0.00355571], error + answer = 0\n",
      "layer: [[1. 1. 1.]]\n",
      "delta for layer 0: [[ 0.00417996 -0.00660217]]\n",
      "weights for layer 0:\n",
      "[[ 0.63325537  2.08015984]\n",
      " [-1.55406436 -1.40590294]\n",
      " [-1.57535424 -1.39950924]]\n",
      "layer: [[ 1.         -0.98651188 -0.62015219]]\n",
      "delta for layer 1: [[-0.00355566]]\n",
      "weights for layer 1:\n",
      "[[-1.04779347]\n",
      " [-2.74245969]\n",
      " [ 2.66728057]]\n",
      "new weights after epoch 4.0:\n",
      "[array([[ 0.63367337,  2.07949963],\n",
      "       [-1.55364636, -1.40656316],\n",
      "       [-1.57493624, -1.40016946]]), array([[-1.04814904],\n",
      "       [-2.74210892],\n",
      "       [ 2.66750108]])]\n",
      "\n",
      "\n",
      "epochs: 5.0\n",
      "[0 0] 0.004781228539230864\n",
      "[0 1] 0.9907923007910061\n",
      "[1 0] 0.9919351701911452\n",
      "[1 1] 0.02126047055484165\n",
      "y before training of epoch 5.0:\n",
      "[array([1., 0., 1.])]\n",
      "(3, 2) (3,)\n",
      "values transmitted by layer 0:\n",
      "[ 1.         -0.73903076  0.59676013]\n",
      "y after last layer:\n",
      "[array([1., 0., 1.]), array([ 1.        , -0.73903076,  0.59676013]), array([0.9907923])]\n",
      "net response error: [0.0092077]\n",
      "1\n",
      "delta vector after epoch 5.0:\n",
      "[array([-0.006752  ,  0.00775772]), array([0.00392147])]\n",
      "net answer: [0.9907923], error + answer = 1\n",
      "layer: [[1. 0. 1.]]\n",
      "delta for layer 0: [[-0.006752    0.00775772]]\n",
      "weights for layer 0:\n",
      "[[ 0.64641201  2.13323836]\n",
      " [-1.60649841 -1.41574842]\n",
      " [-1.59475233 -1.44513819]]\n",
      "layer: [[ 1.         -0.73903076  0.59676013]]\n",
      "delta for layer 1: [[0.00392147]]\n",
      "weights for layer 1:\n",
      "[[-1.06873539]\n",
      " [-2.84636199]\n",
      " [ 2.77047717]]\n",
      "new weights after epoch 5.0:\n",
      "[array([[ 0.64573681,  2.13401413],\n",
      "       [-1.60649841, -1.41574842],\n",
      "       [-1.59542753, -1.44436241]]), array([[-1.06834324],\n",
      "       [-2.8466518 ],\n",
      "       [ 2.77071119]])]\n",
      "\n",
      "\n",
      "epochs: 6.0\n",
      "[0 0] 0.0007297227442376859\n",
      "[0 1] 0.9933998296840154\n",
      "[1 0] 0.9930081915796795\n",
      "[1 1] -0.0007662444180013681\n",
      "y before training of epoch 6.0:\n",
      "[array([1., 0., 0.])]\n",
      "(3, 2) (3,)\n",
      "values transmitted by layer 0:\n",
      "[1.        0.5784281 0.974198 ]\n",
      "y after last layer:\n",
      "[array([1., 0., 0.]), array([1.       , 0.5784281, 0.974198 ]), array([0.00072972])]\n",
      "net response error: [-0.00072972]\n",
      "1\n",
      "delta vector after epoch 6.0:\n",
      "[array([ 0.00155409, -0.00090924]), array([-0.00072972])]\n",
      "net answer: [0.00072972], error + answer = 0\n",
      "layer: [[1. 0. 0.]]\n",
      "delta for layer 0: [[ 0.00155409 -0.00090924]]\n",
      "weights for layer 0:\n",
      "[[ 0.66009721  2.16873274]\n",
      " [-1.61655908 -1.45979758]\n",
      " [-1.63298375 -1.45552388]]\n",
      "layer: [[1.        0.5784281 0.974198 ]]\n",
      "delta for layer 1: [[-0.00072972]]\n",
      "weights for layer 1:\n",
      "[[-1.08687158]\n",
      " [-2.92535568]\n",
      " [ 2.85333088]]\n",
      "new weights after epoch 6.0:\n",
      "[array([[ 0.66025262,  2.16864182],\n",
      "       [-1.61655908, -1.45979758],\n",
      "       [-1.63298375, -1.45552388]]), array([[-1.08694455],\n",
      "       [-2.92539789],\n",
      "       [ 2.85325979]])]\n",
      "\n",
      "\n",
      "epochs: 7.0\n",
      "[0 0] 0.0001800470919018224\n",
      "[0 1] 0.9943555829765053\n",
      "[1 0] 0.9947193815474564\n",
      "[1 1] -0.0013306036132716467\n",
      "y before training of epoch 7.0:\n",
      "[array([1., 0., 1.])]\n",
      "(3, 2) (3,)\n",
      "values transmitted by layer 0:\n",
      "[ 1.         -0.75064453  0.61330296]\n",
      "y after last layer:\n",
      "[array([1., 0., 1.]), array([ 1.        , -0.75064453,  0.61330296]), array([0.99435558])]\n",
      "net response error: [0.00564442]\n",
      "1\n",
      "delta vector after epoch 7.0:\n",
      "[array([-0.00425991,  0.00489276]), array([0.00239095])]\n",
      "net answer: [0.99435558], error + answer = 1\n",
      "layer: [[1. 0. 1.]]\n",
      "delta for layer 0: [[-0.00425991  0.00489276]]\n",
      "weights for layer 0:\n",
      "[[ 0.66934481  2.19865236]\n",
      " [-1.64652722 -1.46790317]\n",
      " [-1.64377472 -1.48445369]]\n",
      "layer: [[ 1.         -0.75064453  0.61330296]]\n",
      "delta for layer 1: [[0.00239095]]\n",
      "weights for layer 1:\n",
      "[[-1.09926456]\n",
      " [-2.9889165 ]\n",
      " [ 2.9175718 ]]\n",
      "new weights after epoch 7.0:\n",
      "[array([[ 0.66891882,  2.19914164],\n",
      "       [-1.64652722, -1.46790317],\n",
      "       [-1.64420071, -1.48396442]]), array([[-1.09902546],\n",
      "       [-2.98909597],\n",
      "       [ 2.91771844]])]\n",
      "\n",
      "\n",
      "epochs: 8.0\n",
      "[0 0] 0.007818098447461853\n",
      "[0 1] 0.9953932082706934\n",
      "[1 0] 0.9956703987115779\n",
      "[1 1] 0.0057866193206133\n",
      "y before training of epoch 8.0:\n",
      "[array([1., 1., 0.])]\n",
      "(3, 2) (3,)\n",
      "values transmitted by layer 0:\n",
      "[ 1.         -0.75702463  0.63046389]\n",
      "y after last layer:\n",
      "[array([1., 1., 0.]), array([ 1.        , -0.75702463,  0.63046389]), array([0.9956704])]\n",
      "net response error: [0.0043296]\n",
      "1\n",
      "delta vector after epoch 8.0:\n",
      "[array([-0.00329007,  0.00374156]), array([0.00183034])]\n",
      "net answer: [0.9956704], error + answer = 1\n",
      "layer: [[1. 1. 0.]]\n",
      "delta for layer 0: [[-0.00329007  0.00374156]]\n",
      "weights for layer 0:\n",
      "[[ 0.67399886  2.22474232]\n",
      " [-1.66320737 -1.48255663]\n",
      " [-1.66063634 -1.49791609]]\n",
      "layer: [[ 1.         -0.75702463  0.63046389]]\n",
      "delta for layer 1: [[0.00183034]]\n",
      "weights for layer 1:\n",
      "[[-1.10744279]\n",
      " [-3.04010761]\n",
      " [ 2.97025161]]\n",
      "new weights after epoch 8.0:\n",
      "[array([[ 0.67366986,  2.22511647],\n",
      "       [-1.66353638, -1.48218247],\n",
      "       [-1.66063634, -1.49791609]]), array([[-1.10725975],\n",
      "       [-3.04024617],\n",
      "       [ 2.97036701]])]\n",
      "\n",
      "\n",
      "epochs: 9.0\n",
      "[0 0] 0.0024887135750609603\n",
      "[0 1] 0.996040083091583\n",
      "[1 0] 0.9961236013791285\n",
      "[1 1] 0.0016541785547388205\n",
      "y before training of epoch 9.0:\n",
      "[array([1., 1., 1.])]\n",
      "(3, 2) (3,)\n",
      "values transmitted by layer 0:\n",
      "[ 1.         -0.99040782 -0.6424253 ]\n",
      "y after last layer:\n",
      "[array([1., 1., 1.]), array([ 1.        , -0.99040782, -0.6424253 ]), array([0.00165418])]\n",
      "net response error: [-0.00165418]\n",
      "1\n",
      "delta vector after epoch 9.0:\n",
      "[array([ 0.00217214, -0.0033823 ]), array([-0.00165417])]\n",
      "net answer: [0.00165418], error + answer = 0\n",
      "layer: [[1. 1. 1.]]\n",
      "delta for layer 0: [[ 0.00217214 -0.0033823 ]]\n",
      "weights for layer 0:\n",
      "[[ 0.68156564  2.2425388 ]\n",
      " [-1.67300903 -1.49835827]\n",
      " [-1.67612984 -1.50647303]]\n",
      "layer: [[ 1.         -0.99040782 -0.6424253 ]]\n",
      "delta for layer 1: [[-0.00165417]]\n",
      "weights for layer 1:\n",
      "[[-1.11573504]\n",
      " [-3.08144739]\n",
      " [ 3.01124562]]\n",
      "new weights after epoch 9.0:\n",
      "[array([[ 0.68178286,  2.24220058],\n",
      "       [-1.67279181, -1.4986965 ],\n",
      "       [-1.67591263, -1.50681125]]), array([[-1.11590046],\n",
      "       [-3.08128356],\n",
      "       [ 3.01135189]])]\n",
      "final prediction\n",
      "[0 0] 0.0007874807630514456\n",
      "[0 1] 0.9966119402723409\n",
      "[1 0] 0.9965050240587418\n",
      "[1 1] -0.0004468875231420363\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 1, verbose\n",
    "np.random.seed(1)\n",
    "nn = NeuralNetwork([2, 2, 1])\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "nn.fit(X, y, epochs=10, verbose=True)\n",
    "\n",
    "print(\"final prediction\")\n",
    "for s in X:\n",
    "    print(s, nn.predict(s))\n",
    "    \n",
    "nn.plot_decision_regions(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "epochs: 0.0\n",
      "[0 0] -0.9333762396355102\n",
      "[0 1] -0.8927209844748133\n",
      "[1 0] -0.9039355440054777\n",
      "[1 1] -0.8384376643400449\n",
      "\n",
      "\n",
      "epochs: 1.0\n",
      "[0 0] -0.011541934476393042\n",
      "[0 1] 0.8436336213348098\n",
      "[1 0] 0.8833500834329784\n",
      "[1 1] -0.0037336392896542177\n",
      "\n",
      "\n",
      "epochs: 2.0\n",
      "[0 0] 0.010795713690083857\n",
      "[0 1] 0.9609143057380306\n",
      "[1 0] 0.976579932482217\n",
      "[1 1] -0.0028805474264308544\n",
      "\n",
      "\n",
      "epochs: 3.0\n",
      "[0 0] 0.009448081952927203\n",
      "[0 1] 0.9817054754607706\n",
      "[1 0] 0.9872528150582088\n",
      "[1 1] 0.014914379359901914\n",
      "\n",
      "\n",
      "epochs: 4.0\n",
      "[0 0] 0.01435767288754408\n",
      "[0 1] 0.9883597591467227\n",
      "[1 0] 0.9907922160162363\n",
      "[1 1] 0.0027622027631696534\n",
      "\n",
      "\n",
      "epochs: 5.0\n",
      "[0 0] 0.003805335841626724\n",
      "[0 1] 0.9909823498096653\n",
      "[1 0] 0.9942201094620315\n",
      "[1 1] 0.024947636331815317\n",
      "\n",
      "\n",
      "epochs: 6.0\n",
      "[0 0] 0.00012614180619855545\n",
      "[0 1] 0.9932372494982287\n",
      "[1 0] 0.9948320303441583\n",
      "[1 1] 0.0014799542962555007\n",
      "\n",
      "\n",
      "epochs: 7.0\n",
      "[0 0] 0.00611660822846499\n",
      "[0 1] 0.9943513752251579\n",
      "[1 0] 0.9961747680121966\n",
      "[1 1] 0.01085689986063549\n",
      "\n",
      "\n",
      "epochs: 8.0\n",
      "[0 0] -2.7042834632151023e-05\n",
      "[0 1] 0.9950756559168209\n",
      "[1 0] 0.9966951517581879\n",
      "[1 1] -0.006418034099810719\n",
      "\n",
      "\n",
      "epochs: 9.0\n",
      "[0 0] 0.006129924289775715\n",
      "[0 1] 0.9958687830592511\n",
      "[1 0] 0.9970478163096771\n",
      "[1 1] 0.0010399328574472505\n",
      "final prediction\n",
      "[0 0] 0.0009337701362651419\n",
      "[0 1] 0.996444637673525\n",
      "[1 0] 0.997315546811523\n",
      "[1 1] 0.004450987369719703\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 2, larger hidden neuron to show how the decision boundaries change\n",
    "np.random.seed(1)\n",
    "nn = NeuralNetwork([2, 6, 1])\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "nn.fit(X, y, epochs=10, verbose=False)\n",
    "\n",
    "print(\"final prediction\")\n",
    "for s in X:\n",
    "    print(s, nn.predict(s))\n",
    "    \n",
    "nn.plot_decision_regions(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}