{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Python Deep Learning\n",
    "# Ivan Vasilev, Daniel Slater, Gianmario Spacagna, Peter Roelants, Valentino Zocca\n",
    "## Second Edition - Packt \n",
    "# Notes / Experiments with Python (and PyTorch, TensorFlow and Keras, among others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and plot settings\n",
    "PLT_TYPE = 'inline'  # restart Jupyter kernel before switching back to 'inline'\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "if PLT_TYPE.lower() == 'widget':\n",
    "    %matplotlib widget\n",
    "elif PLT_TYPE.lower() == 'inline':\n",
    "    pass\n",
    "elif PLT_TYPE.lower() == 'window':\n",
    "    matplotlib.use('Qt5Agg')\n",
    "    plt.ion()\n",
    "else:\n",
    "    raise SyntaxError(\"PLT_TYPE can either be 'inline', 'widget' or 'window\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Chapter 1. Machine Learning - an Introduction\n",
    "* AI can be defined, among other things, as a system that interact with its environment, using sensors and actuators.\n",
    "* Machine learning can be seen as the way an AI tries and formulate appropriate answers as a function of the data available.\n",
    "* Deep learning is a subfield of machine learning, mainly defined by the use of certain techniques/approaches. The most \n",
    "representative class of these methods are deep neural networks.\n",
    "\n",
    "### Approaches to learning\n",
    "* Supervised learning\n",
    "* Unsupervised learning\n",
    "* Reinforcement learning\n",
    "\n",
    "### Supervised Learning\n",
    "Supervised learning involves the use of previously-labeled data in order to learn its features, so they can then classify\n",
    "new, similar but unlabeled data. This approaches needs **training data**. \n",
    "\n",
    "One way of thinking of supervised learning is as a function _f_ defined over a dataset, which comprises information organized\n",
    "by **features**. \n",
    "\n",
    "```f: space of features -> classes = (discrete values or real values)``` \n",
    "\n",
    "We will use the ```MNIST``` dataset (Modified National Institute of Standard and Technology) to work on handwritten digits\n",
    "recognition by means of either classification or regression approaches. In this case, we'll use images of 28x28 pixels of\n",
    "size. **Our algorithm will use a 28x28 = 784-dimensional feature space to classify the digits**.\n",
    "\n",
    "#### Linear and Logistic Regression\n",
    "Regression algorithms are a type of supervised algos that uses features of the input data to predict a value, such as the\n",
    "cost of a certain type of good, given a feature set. Regression tries to find the value of the parameters for the function\n",
    "that best fits an input dataset.\n",
    "\n",
    "In linear regression algos, the goal is to minimize the cost function (error of prediction from truth) by finding appropriate\n",
    "values for the parameters of the function, over the input data that best approximates the target value. A popular example\n",
    "is **mean square error** (MSE). A pseudo-code representation of this would be:\n",
    "```\n",
    "Given:\n",
    "* a dataset comprising input/target pairs as (x[i], t[i]) for every i in range(len(dataset))\n",
    "* a vector w containing random values of len(w) == len(features) == len(x[i])\n",
    "For an arbitrarily large number of times, repeat:\n",
    "    Err = 0  # initialize cost\n",
    "    for every i in range(len(dataset))\n",
    "        Err += (x[i] * w - t[i]) ** 2\n",
    "    MSE = Err / len(dataset)\n",
    "```\n",
    "\n",
    "We iterate over the training data to compute the cost function to obtain the MSE, and then we use the gradient-descent\n",
    "algorithm to update _w_ accordingly. This involves computing the derivatives of the cost function with respect of each\n",
    "weight, in order to determine how the cost changes with respect of each weight. We'll see how this is remarkably similar\n",
    "to the process used to train neural networks.\n",
    "\n",
    "We can adapt the regression logic to a situation in which we want to reduce the outcome of our algorithm to a discrete,\n",
    "categorical output (as opposed to a real-value output, such as the cost of a given good above). In this case we can use\n",
    "**logistic regression**. It is imaginable as a probability between two certain possible outcomes, and the response label\n",
    "is either one or the other possible outcome. To make this technique usable for classification problems, we need to introduce\n",
    "a rule that determines the class based on the logistic function output (i.e. similar to a threshold). Boh.\n",
    "\n",
    "#### Support Vector Machines\n",
    "A support vector machine is a type of supervised machine learning algo used mainly for classification. It belongs\n",
    "to the kernel method class of algos. An SVM tries to find a hyperplane separating the sample across its features.\n",
    "\n",
    "#### Decision Trees\n",
    "A decision tree takes on classification problems by representing the whole computation and decision process as a tree.\n",
    "It is composed of decision nodes, in charge of testing specific attributes of the data, and leaf nodes, indicating the \n",
    "value of the target attribute. To begin a classification, we start at the root and navigate down the nodes until we \n",
    "reach a leaf. The _Iris flower dataset_ can be used to show this algorithm. We can create a decision tree to decide which\n",
    "species a certain flower belongs to:\n",
    "```\n",
    "          Petal Length < 2.5\n",
    "                  |\n",
    "    *Iris Setosa* - Petal Width < 1.8\n",
    "                           |                 \n",
    "        Petal Length < 4.9 - *Iris Virginica*\n",
    "                  |\n",
    "*Iris Versicolor* - *Iris Virginica*\n",
    "```\n",
    "#### Naive Bayes\n",
    "No, I'll come here later I think.\n",
    "\n",
    "### Unsupervised Learning\n",
    "This class involves methods that try to come to its own conclusions about the data without labels/ground truth. With\n",
    " **cluster analysis**, we try finding \"natural\" clustering behaviors in the data, given certain features, to derive\n",
    "the different classes possibly underlying the data (_k-means_ are an example of this subclass).\n",
    "\n",
    "A different approach, **recurrent neural networks**, make use of the \"context\" of data (i.e., in natural language processing,\n",
    "each word in a phrase is submitted, together with its neighboring words - the context - to simple neural nets). \n",
    "\n",
    "With **generative adversarial networks (GANs)**, we first train a network with large dataset, and then we use the network\n",
    "to produce new examples similar to the training dataset. They can be used to colorize black and white photographs, alter\n",
    "facial expressions in images and more.\n",
    "\n",
    "### Reinforcement Learning\n",
    "This third class involves having an algorithm try to maximize certain rewards obtained by interacting with an environment.\n",
    "The _agent_ takes an action that changes the state of the environment. It then uses the new state and the reward to \n",
    "determine its next action. It has to do with using previous, progressively accumulated experience to improve in the task,\n",
    "as opposed to just using ground truths to derive rules. \n",
    "\n",
    "#### Q-learning\n",
    "Q-learning is an off-policy temporal-difference reinforcement learning algorithm. A suitable example is imagining trying\n",
    "to create an ML agent that plays and tries to win a chess game. For any given time, the state of the chess game\n",
    "is represented by the board configuration (i.e. the location of the pieces on the board). The agent needs then to take an\n",
    "action, a, by moving a piece, thus changing the state of the board. The problem may be represented as a graph. Each\n",
    "vertex represents a given board configuration, while each edge is a move that brings the state from one configuration of\n",
    "the pieces (node) to another. By making a move, the agent moves from one edge to another, and uses a _Q-table_ to decide\n",
    "which move to take. A Q-table has one row for each different board configuration (state, ```s```), and a column for each\n",
    "possible action that the agent can take (a move, ```a```). A given cell of the Q-table, ```q(s, a)```, contains the \n",
    "potential total reward obtained for the remainder of the game if the agent takes an action ```a``` from the current state\n",
    "```s``` and it's called **Q-value**. The Q-table is first initialized with arbitrary values, and it's then filled as the\n",
    "game progresses and finishes. The Q-values are used to determine the \"attractiveness\" of a certain move in a certain state\n",
    "in the attempt to ultimately win the game. It allows the agent to seek high potential rewards by exploring.\n",
    "\n",
    "### Components of an ML solution\n",
    "* Learner\n",
    "* Training data\n",
    "* Representation (how we express data in terms of selected features to provide the learner with)\n",
    "* Goal (the reason to learn from the data, the aim)\n",
    "* Target (what is being learned as well as the final output)\n",
    "\n",
    "#### Creation of the test case\n",
    "* Training set (what we use in training phase)\n",
    "* Validation set (to evaluate the accuracy of the algo using unknown data. Sometimes we can fine-tune the model after\n",
    "feedback from using the validation set, and it is used to determine when to stop learning)\n",
    "* Test set (to use only **once** after training, to prevent introducing bias when over-tuning the data in successive\n",
    "attempts with the same data points)\n",
    "\n",
    "...\n",
    "\n",
    "### Introduction to PyTorch\n",
    "Finally!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "Epoch 1 Loss: 1.2181\n",
      "Epoch 10 Loss: 0.6745\n",
      "Epoch 20 Loss: 0.2447\n",
      "Epoch 30 Loss: 0.1397\n",
      "Epoch 40 Loss: 0.1001\n",
      "Epoch 50 Loss: 0.0855\n",
      "Errors: 0, Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "# Download and parse a CSV file from the Internet, containing 150 rows of the IRIS flower dataset\n",
    "my_dataset = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \n",
    "                         names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "# Recode the categorical column 'species' from strings to codes ([0, 1, 2]) for the 3 flowers\n",
    "my_dataset['species'] = pd.Categorical(my_dataset['species']).codes\n",
    "# Shuffle the order of the rows of the dataset\n",
    "my_dataset = my_dataset.sample(frac=1, random_state=1234)\n",
    "\n",
    "print(len(my_dataset.values))\n",
    "train_input = my_dataset.values[:120, :4]  # The majority of the rows will be training dataset, exclude the column containing the ground truth\n",
    "train_target = my_dataset.values[:120, 4]  # Store the ground truth only for 120 of the rows, like above\n",
    "\n",
    "test_input = my_dataset.values[120:, :4]  # Same for the test dataset, only using the last 30 rows for this\n",
    "test_target = my_dataset.values[120:, 4]\n",
    "\n",
    "torch.manual_seed(1234)  # reproducibility\n",
    "\n",
    "hidden_units = 5  # One hidden layer containing 5 neurons\n",
    "\n",
    "# Feed-forward network, 1 hidden layer (5 units), rectified linear activation function and 1 output layer with 3 units\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, hidden_units),  # first layer, 4 neurons, one per variable of interest (sepal_length, sepal_width, etc.)\n",
    "    torch.nn.ReLU(),  # rectified linear activation function\n",
    "    torch.nn.Linear(hidden_units, 3)  # output layer, 3 units (one per possible flower type)\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "my_crit = torch.nn.CrossEntropyLoss()  # cross entropy loss as loss function\n",
    "my_optim = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)  # stochastic gradient descent as optimizer\n",
    "\n",
    "# training loop\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # noinspection PyArgumentList\n",
    "    inputs = torch.autograd.Variable(torch.Tensor(train_input).float())\n",
    "    # noinspection PyArgumentList\n",
    "    targets = torch.autograd.Variable(torch.Tensor(train_target).long())\n",
    "    \n",
    "    my_optim.zero_grad()  # Start with fresh zeroed gradients for the current epoch\n",
    "    out = net(inputs)  # submit the inputs to the net\n",
    "    loss = my_crit(out, targets)  # how far were we from the truth?\n",
    "    loss.backward()  # Compute contributions of each weight\n",
    "    my_optim.step()  # attempt improvement\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print('Epoch %d Loss: %.4f' % (epoch + 1, loss.item()))\n",
    "        \n",
    "# noinspection PyArgumentList\n",
    "inputs = torch.autograd.Variable(torch.Tensor(test_input).float())\n",
    "# noinspection PyArgumentList\n",
    "targets = torch.autograd.Variable(torch.Tensor(test_target).long())\n",
    "\n",
    "my_optim.zero_grad()\n",
    "out = net(inputs)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "error_count = test_target.size - np.count_nonzero((targets == predicted).numpy())\n",
    "# noinspection PyTypeChecker\n",
    "print('Errors: %d, Accuracy: %d%%' % (error_count, 100 * torch.sum(targets == predicted) / test_target.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It worked perfectly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}