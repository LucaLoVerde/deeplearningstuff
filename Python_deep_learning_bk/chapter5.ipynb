{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Chapter 5. Advanced Computer Vision\n",
    "### Transfer learning\n",
    "We are going to try and re-purpose a pre-trained ImageNet nn on the CIFAR-10 dataset. We'll be using pytorch to show two\n",
    "different techniques, **feature extraction** and **fine tuning**.\n",
    "#### Feature extraction\n",
    "Feature extraction involves modifying the final layer of the existing net to accommodate the new task. Every weight of\n",
    "the network, except for the final, modified layer, are locked and will not change during the training procedure. This is \n",
    "shown below in the ```t1_feature_extractor()``` function.\n",
    "\n",
    "#### Fine tuning\n",
    "Fine tuning involves modifying the final layer of the existing net to accommodate the new task and then training the whole\n",
    "network, starting from the pre-trained weights. This is shown below in the ```t1_fine_tuning()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "C:\\Users\\Luca\\PycharmProjects\\deeplearningstuff\\Python_deep_learning_bk\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "********** BEGINNING FEATURE EXTRACTION PROCEDURE **********\n",
      "\n",
      "Epoch 1/5\n",
      "train loss: 1.0433; accuracy: 0.6446\n",
      "test loss: 0.8522; accuracy: 0.7046\n",
      "Epoch 2/5\n",
      "train loss: 0.8525; accuracy: 0.7029\n",
      "test loss: 0.8256; accuracy: 0.7110\n",
      "Epoch 3/5\n",
      "train loss: 0.8311; accuracy: 0.7100\n",
      "test loss: 0.8349; accuracy: 0.7046\n",
      "Epoch 4/5\n",
      "train loss: 0.8193; accuracy: 0.7124\n",
      "test loss: 0.8167; accuracy: 0.7152\n",
      "Epoch 5/5\n",
      "train loss: 0.8171; accuracy: 0.7152\n",
      "test loss: 0.7839; accuracy: 0.7223\n",
      "\n",
      "********** FEATURE EXTRATION PROCEDURE COMPLETED **********\n",
      "\n",
      "\n",
      "********** BEGINNING FINE TUNING PROCEDURE **********\n",
      "\n",
      "epoch: 1/5\n",
      "train loss: 0.8025; accuracy: 0.7194\n",
      "test loss: 0.7358; accuracy: 0.7384\n",
      "epoch: 2/5\n",
      "train loss: 0.5268; accuracy: 0.8188\n",
      "test loss: 0.5423; accuracy: 0.8189\n",
      "epoch: 3/5\n",
      "train loss: 0.4305; accuracy: 0.8508\n",
      "test loss: 0.4316; accuracy: 0.8576\n",
      "epoch: 4/5\n",
      "train loss: 0.3656; accuracy: 0.8729\n",
      "test loss: 0.3813; accuracy: 0.8661\n",
      "epoch: 5/5\n",
      "train loss: 0.3209; accuracy: 0.8888\n",
      "test loss: 0.3757; accuracy: 0.8716\n",
      "\n",
      "********** FINE TUNING PROCEDURE COMPLETED **********\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# TODO: comment everything\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "print(os.getcwd())\n",
    "batch_size = 50\n",
    "# training data\n",
    "train_data_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_data_transform)\n",
    "\n",
    "from torch.utils.data.dataloader import  DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "val_data_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=val_data_transform)\n",
    "\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model: nn.Module, loss_function: nn.CrossEntropyLoss, optimizer: torch.optim.Adam, data_loader: torch.utils.data.DataLoader):\n",
    "    model.train()\n",
    "    \n",
    "    current_loss = 0.0\n",
    "    current_acc = 0.0\n",
    "    \n",
    "    # iterate\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        current_loss += loss.item() * inputs.size(0)\n",
    "        hits = predictions == labels.data\n",
    "        current_acc += torch.sum(hits)\n",
    "        \n",
    "    total_loss = current_loss / len(data_loader.dataset)\n",
    "    total_acc = current_acc / len(data_loader.dataset)\n",
    "    \n",
    "    print('train loss: {:.4f}; accuracy: {:.4f}'.format(total_loss, total_acc))\n",
    "    \n",
    "    \n",
    "def test_model(model: nn.Module, loss_function: nn.CrossEntropyLoss, data_loader: torch.utils.data.DataLoader):\n",
    "    model.eval()\n",
    "    \n",
    "    current_loss = 0.0\n",
    "    current_acc = 0.0\n",
    "    \n",
    "    # iterate\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            \n",
    "        current_loss += loss.item() * inputs.size(0)\n",
    "        hits = predictions == labels.data\n",
    "        current_acc += torch.sum(hits)\n",
    "        \n",
    "    total_loss = current_loss / len(data_loader.dataset)\n",
    "    total_acc = current_acc / len(data_loader.dataset)\n",
    "    print('test loss: {:.4f}; accuracy: {:.4f}'.format(total_loss, total_acc))\n",
    "    \n",
    "    \n",
    "def t1_feature_extractor(epochs: int = 3):\n",
    "    print('\\n********** BEGINNING FEATURE EXTRACTION PROCEDURE **********\\n')\n",
    "    model: nn.Module = torchvision.models.resnet18(pretrained=True)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 10)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim.Adam(model.fc.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        \n",
    "        train_model(model, loss_function, optimizer, train_loader)\n",
    "        test_model(model, loss_function, val_loader)\n",
    "    \n",
    "    print('\\n********** FEATURE EXTRACTION PROCEDURE COMPLETED **********\\n')\n",
    "    \n",
    "    \n",
    "def t1_fine_tuning(epochs: int = 3):\n",
    "    print('\\n********** BEGINNING FINE TUNING PROCEDURE **********\\n')\n",
    "    model: nn.Module = models.resnet18(pretrained=True)\n",
    "        \n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 10)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('epoch: {}/{}'.format(epoch + 1, epochs))\n",
    "        \n",
    "        train_model(model, loss_function, optimizer, train_loader)\n",
    "        test_model(model, loss_function, val_loader)\n",
    "        \n",
    "    print('\\n********** FINE TUNING PROCEDURE COMPLETED **********\\n')\n",
    "\n",
    "# EXECUTE\n",
    "t1_feature_extractor(5)\n",
    "t1_fine_tuning(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "pag 125"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}