{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Python Deep Learning\n",
    "# Ivan Vasilev, Daniel Slater, Gianmario Spacagna, Peter Roelants, Valentino Zocca\n",
    "## Second Edition - Packt \n",
    "# Notes / Experiments with Python (and PyTorch, TensorFlow and Keras, among others)\n",
    "\n",
    "## Chapter 1. Machine Learning - an Introduction\n",
    "* AI can be defined, among other things, as a system that interact with its environment, using sensors and actuators.\n",
    "* Machine learning can be seen as the way an AI tries and formulate appropriate answers as a function of the data available.\n",
    "* Deep learning is a subfield of machine learning, mainly defined by the use of certain techniques/approaches. The most \n",
    "representative class of these methods are deep neural networks.\n",
    "\n",
    "### Approaches to learning\n",
    "* Supervised learning\n",
    "* Unsupervised learning\n",
    "* Reinforcement learning\n",
    "\n",
    "### Supervised Learning\n",
    "Supervised learning involves the use of previously-labeled data in order to learn its features, so they can then classify\n",
    "new, similar but unlabeled data. This approaches needs **training data**. \n",
    "\n",
    "One way of thinking of supervised learning is as a function _f_ defined over a dataset, which comprises information organized\n",
    "by **features**. \n",
    "\n",
    "```f: space of features -> classes = (discrete values or real values)``` \n",
    "\n",
    "We will use the ```MNIST``` dataset (Modified National Institute of Standard and Technology) to work on handwritten digits\n",
    "recognition by means of either classification or regression approaches. In this case, we'll use images of 28x28 pixels of\n",
    "size. **Our algorithm will use a 28x28 = 784-dimensional feature space to classify the digits**.\n",
    "\n",
    "#### Linear and Logistic Regression\n",
    "Regression algorithms are a type of supervised algos that uses features of the input data to predict a value, such as the\n",
    "cost of a certain type of good, given a feature set. Regression tries to find the value of the parameters for the function\n",
    "that best fits an input dataset.\n",
    "\n",
    "In linear regression algos, the goal is to minimize the cost function (error of prediction from truth) by finding appropriate\n",
    "values for the parameters of the function, over the input data that best approximates the target value. A popular example\n",
    "is **mean square error** (MSE). A pseudo-code representation of this would be:\n",
    "```\n",
    "Given:\n",
    "* a dataset comprising input/target pairs as (x[i], t[i]) for every i in range(len(dataset))\n",
    "* a vector w containing random values of len(w) == len(features) == len(x[i])\n",
    "For an arbitrarily large number of times, repeat:\n",
    "    Err = 0  # initialize cost\n",
    "    for every i in range(len(dataset))\n",
    "        Err += (x[i] * w - t[i]) ** 2\n",
    "    MSE = Err / len(dataset)\n",
    "```\n",
    "\n",
    "We iterate over the training data to compute the cost function to obtain the MSE, and then we use the gradient-descent\n",
    "algorithm to update _w_ accordingly. This involves computing the derivatives of the cost function with respect of each\n",
    "weight, in order to determine how the cost changes with respect of each weight. We'll see how this is remarkably similar\n",
    "to the process used to train neural networks.\n",
    "\n",
    "We can adapt the regression logic to a situation in which we want to reduce the outcome of our algorithm to a discrete,\n",
    "categorical output (as opposed to a real-value output, such as the cost of a given good above). In this case we can use\n",
    "**logistic regression**. It is imaginable as a probability between two certain possible outcomes, and the response label\n",
    "is either one or the other possible outcome. To make this technique usable for classification problems, we need to introduce\n",
    "a rule that determines the class based on the logistic function output (i.e. similar to a threshold). Boh.\n",
    "\n",
    "#### Support Vector Machines\n",
    "A support vector machine is a type of supervised machine learning algo used mainly for classification. It belongs\n",
    "to the kernel method class of algos. An SVM tries to find a hyperplane separating the sample across its features.\n",
    "\n",
    "#### Decision Trees\n",
    "A decision tree takes on classification problems by representing the whole computation and decision process as a tree.\n",
    "It is composed of decision nodes, in charge of testing specific attributes of the data, and leaf nodes, indicating the \n",
    "value of the target attribute. To begin a classification, we start at the root and navigate down the nodes until we \n",
    "reach a leaf. The _Iris flower dataset_ can be used to show this algorithm. We can create a decision tree to decide which\n",
    "species a certain flower belongs to:\n",
    "```\n",
    "          Petal Length < 2.5\n",
    "                  |\n",
    "    *Iris Setosa* - Petal Width < 1.8\n",
    "                           |                 \n",
    "        Petal Length < 4.9 - *Iris Virginica*\n",
    "                  |\n",
    "*Iris Versicolor* - *Iris Virginica*\n",
    "```\n",
    "#### Naive Bayes\n",
    "No, I'll come here later I think.\n",
    "\n",
    "### Unsupervised Learning\n",
    "This class involves methods that try to come to its own conclusions about the data without labels/ground truth. With\n",
    " **cluster analyisis**, we try finding \"natural\" clustering behaviors in the data, given certain features, to derive\n",
    "the different classes possibly underlying the data (_k-means_ are an example of this subclass).\n",
    "\n",
    "A different approach, **recurrent neural networks**, make use of the \"context\" of data (i.e., in natural language processing,\n",
    "each word in a phrase is submitted, together with its neighboring words - the context - to simple neural nets). \n",
    "\n",
    "With **generative adversarial networks (GANs)**, we first train a network with large dataset, and then we use the network\n",
    "to produce new examples similar to the training dataset. They can be used to colorize black and white photographs, alter\n",
    "facial expressions in images and more.\n",
    "\n",
    "### Reinforcement Learning\n",
    "This third class involves having an algorithm try to maximize certain rewards obtained by interacting with an environment.\n",
    "The _agent_ takes an action that changes the state of the environment. It then uses the new state and the reward to \n",
    "determine its next action. It has to do with using previous, progressively accumulated experience to improve in the task,\n",
    "as opposed to just using ground truths to derive rules. \n",
    "\n",
    "#### Q-learning\n",
    "Q-learning is an off-policy temporal-difference reinforcement learning algorithm. A suitable example is imagining trying\n",
    "to create an ML agent that plays and tries to win a chess game. For any given time, the state of the chess game\n",
    "is represented by the board configuration (i.e. the location of the pieces on the board). The agent needs then to take an\n",
    "action, a, by moving a piece, thus changing the state of the board. The problem may be represented as a graph. Each\n",
    "vertex represents a given board configuration, while each edge is a move that brings the state from one configuration of\n",
    "the pieces (node) to another. By making a move, the agent moves from one edge to another, and uses a _Q-table_ to decide\n",
    "which move to take. A Q-table has one row for each different board configuration (state, ```s```), and a column for each\n",
    "possible action that the agent can take (a move, ```a```). A given cell of the Q-table, ```q(s, a)```, contains the \n",
    "potential total reward obtained for the remainder of the game if the agent takes an action ```a``` from the current state\n",
    "```s``` and it's called **Q-value**. The Q-table is first initialized with arbitrary values, and it's then filled as the\n",
    "game progresses and finishes. The Q-values are used to determine the \"attractiveness\" of a certain move in a certain state\n",
    "in the attempt to ultimately win the game. It allows the agent to seek high potential rewards by exploring.\n",
    "\n",
    "### Components of an ML solution\n",
    "* Learner\n",
    "* Training data\n",
    "* Representation (how we express data in terms of selected features to provide the learner with)\n",
    "* Goal (the reason to learn from the data, the aim)\n",
    "* Target (what is being learned as well as the final output)\n",
    "\n",
    "#### Creation of the test case\n",
    "* Training set (what we use in training phase)\n",
    "* Validation set (to evaluate the accuracy of the algo using unknown data. Sometimes we can fine-tune the model after\n",
    "feedback from using the validation set, and it is used to determine when to stop learning)\n",
    "* Test set (to use only **once** after training, to prevent introducing bias when over-tuning the data in successive\n",
    "attempts with the same data points)\n",
    "\n",
    "...\n",
    "\n",
    "### Introduction to PyTorch\n",
    "Finally!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "Epoch 1 Loss: 1.2181\n",
      "Epoch 10 Loss: 0.6745\n",
      "Epoch 20 Loss: 0.2447\n",
      "Epoch 30 Loss: 0.1397\n",
      "Epoch 40 Loss: 0.1001\n",
      "Epoch 50 Loss: 0.0855\n",
      "Errors: 0, Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Download and parse a CSV file from the Internet, containing 150 rows of the IRIS flower dataset\n",
    "my_dataset = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \n",
    "                         names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "# Recode the categorical column 'species' from strings to codes ([0, 1, 2]) for the 3 flowers\n",
    "my_dataset['species'] = pd.Categorical(my_dataset['species']).codes\n",
    "# Shuffle the order of the rows of the dataset\n",
    "my_dataset = my_dataset.sample(frac=1, random_state=1234)\n",
    "\n",
    "print(len(my_dataset.values))\n",
    "train_input = my_dataset.values[:120, :4]  # The majority of the rows will be training dataset, exclude the column containing the ground truth\n",
    "train_target = my_dataset.values[:120, 4]  # Store the ground truth only for 120 of the rows, like above\n",
    "\n",
    "test_input = my_dataset.values[120:, :4]  # Same for the test dataset, only using the last 30 rows for this\n",
    "test_target = my_dataset.values[120:, 4]\n",
    "\n",
    "torch.manual_seed(1234)  # reproducibility\n",
    "\n",
    "hidden_units = 5  # One hidden layer containing 5 neurons\n",
    "\n",
    "# Feed-forward network, 1 hidden layer (5 units), rectified linear activation function and 1 output layer with 3 units\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, hidden_units),  # first layer, 4 neurons, one per variable of interest (sepal_length, sepal_width, etc.)\n",
    "    torch.nn.ReLU(),  # rectified linear activation function\n",
    "    torch.nn.Linear(hidden_units, 3)  # output layer, 3 units (one per possible flower type)\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "my_crit = torch.nn.CrossEntropyLoss()  # cross entropy loss as loss function\n",
    "my_optim = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)  # stochastic gradient descent as optimizer\n",
    "\n",
    "# training loop\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    inputs = torch.autograd.Variable(torch.Tensor(train_input).float())\n",
    "    targets = torch.autograd.Variable(torch.Tensor(train_target).long())\n",
    "    \n",
    "    my_optim.zero_grad()  # Start with fresh zeroed gradients for the current epoch\n",
    "    out = net(inputs)  # submit the inputs to the net\n",
    "    loss = my_crit(out, targets)  # how far were we from the truth?\n",
    "    loss.backward()  # Compute contributions of each weight\n",
    "    my_optim.step()  # attempt improvement\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print('Epoch %d Loss: %.4f' % (epoch + 1, loss.item()))\n",
    "        \n",
    "inputs = torch.autograd.Variable(torch.Tensor(test_input).float())\n",
    "targets = torch.autograd.Variable(torch.Tensor(test_target).long())\n",
    "\n",
    "my_optim.zero_grad()\n",
    "out = net(inputs)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "\n",
    "error_count = test_target.size - np.count_nonzero((targets == predicted).numpy())\n",
    "print('Errors: %d, Accuracy: %d%%' % (error_count, 100 * torch.sum(targets == predicted) / test_target.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It worked perfectly.\n",
    "## 2. Neural Networks\n",
    "### Intro to neural networks\n",
    "* Information occurs mathematically, over simple elements called **neurons**\n",
    "* Neurons are connected and exchange signals (i.e. numbers) between each other through links\n",
    "* Each link has a **weight**, determining how information is processed as it passes through\n",
    "* Each neuron has an internal state, determined by all the incoming links\n",
    "* Each neuron has an **activation function** which determines the output signal as a function of its state\n",
    "\n",
    "The **architecture** of a neural network defines what type of connectivity the units have (i.e. feedforward, recurrent, multi-\n",
    "or single-layered, etc.), the number of layers and of neurons within each layer.\n",
    "The **learning** describes how the network adapt and improves; common tools are _gradient descent_ and _backpropagation_.\n",
    "\n",
    "### Intro to neurons\n",
    "Mathematically, a neuron is defined as:\n",
    "```\n",
    "y = f(sum(x[:] * w[:]) + b)\n",
    "```\n",
    "where ```x``` represents the inputs, and ```w``` represents the weights. ```b``` represents the _bias_ and its input is \n",
    "always 1.\n",
    "\n",
    "### Intro to layers\n",
    "In a neural network, the input layer represents the dataset and the initial conditions: if the net deals with grayscale \n",
    "images, the units in the first layer will represent the pixel intensity. The output layer can have more than one neuron:\n",
    "usually we find one unit per possible answer/class.\n",
    "\n",
    "### Multi-layer networks\n",
    "Single layer networks can only classify linearly separable classes, but by adding **hidden layers** we can surpass this\n",
    "limitation. Another condition for multi-layer network to classify linearly-inseparable classes is that their **activation\n",
    "function must not be linear**.\n",
    "\n",
    "### Different types of activation function\n",
    "The most common activation functions are the following:\n",
    "* ```f(a) = a``` - **identity function**\n",
    "* ```f(a) = [1 if a >= 0, otherwise 0]``` - **threshold activity function**\n",
    "* ```f(a) = 1 / (1 + exp(-a))``` - **logistic function**, or **logistic sigmoid** (0, 1)\n",
    "* ```f(a) = 2 / (1 + exp(-a)) = (1 - exp(-a)) / (1 + exp(-a))``` - **bipolar sigmoid** (-1, 1)\n",
    "* ```f(a) = (exp(a)-exp(-a)) / (exp(a) + exp(-a)) = (1 - exp(-2 * a)) / (1 + exp(-2 * a))``` - **hyperbolic tangent**, tanh (-1, 1)\n",
    "* ``` f(a) = [a if a >= 0, otherwise 0]``` - **rectified linear unit**, **ReLU** (0, Inf)\n",
    "\n",
    "### Example\n",
    "Let's build a very simple net, one hidden layer with two neurons, and single input and output neurons. It will support\n",
    "the idea that under the _Universal Approximation Theorem_ any continuous function on compact subsets of Rn can be \n",
    "approximated by a neural network with at least one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2217dd66c0dc48789f61e3d542f0457c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The step function starts at -5.0 and ends at 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\Anaconda3\\envs\\py37ML\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\Luca\\Anaconda3\\envs\\py37ML\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "INLINE_PLOTS = True  # restart Jupyter kernel before resetting it to True (Jupyter detail)\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "if not INLINE_PLOTS:\n",
    "    matplotlib.use('Qt5Agg')\n",
    "    plt.ion()\n",
    "\n",
    "weight_value = 1000\n",
    "\n",
    "bias_value_1 = 5000\n",
    "\n",
    "bias_value_2 = -5000\n",
    "\n",
    "plt.axis([-10, 10, -1, 10])\n",
    "\n",
    "print(\"The step function starts at {0} and ends at {1}\".format(-bias_value_1 / weight_value, -bias_value_2 / weight_value))\n",
    "\n",
    "inputs = np.arange(-10, 10, 0.01)\n",
    "outputs = list()\n",
    "\n",
    "for x in inputs:\n",
    "    y1 = 1.0 / (1.0 + np.exp(-weight_value * x - bias_value_1))\n",
    "    y2 = 1.0 / (1.0 + np.exp(-weight_value * x - bias_value_2))\n",
    "    \n",
    "    w = 7\n",
    "    \n",
    "    y = y1 * w - y2 * w\n",
    "    \n",
    "    outputs.append(y)\n",
    "plt.plot(inputs, outputs, lw=2, color='black')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Pyplot backends and settings\n",
    "Importing ```matplotlib``` and calling the method ```matplotlib.use()``` allows to change pyplot backend.\n",
    "* **default** DON'T KNOW \n",
    "* **'WebAgg'** opens a new browser tab, interactive\n",
    "* **'Qt5Agg'** new window, interactive (use ```plt.show(block=False)```!!!)\n",
    "\n",
    "Beware: after setting the backend to ```Qt5Agg``` (or any interactive one), to have inline plots in the notebook again\n",
    "the Jupyter kernel must be restarted before disabling the ```matplotlib.use()``` call.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
