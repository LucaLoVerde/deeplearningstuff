{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Programming PyTorch for Deep Learning\n",
    "### Ian Pointer (O'Reilly)\n",
    "### Notes and tests\n",
    "\n",
    "This books assumes a working CUDA installation. Let's hope for the best...\n",
    "\n",
    "## Chapter 1. Getting started with PyTorch\n",
    "### Tensors\n",
    "A tensor is both a container for numbers (like a vector or matrix) but also represents sets of rules defining transformations\n",
    "between tensors that produce new tensors. Tensors have ranks that represent their dimensional space. Tensors with PyTorch\n",
    "support fetching and changing elements using the standard Python indexing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor(3)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch, torchvision, numpy, pandas\n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(x)\n",
    "print(x[0][-1])  # fetch the last element of the first dimension\n",
    "x[0][0] = 10  # change the value of the first element of the first dimension\n",
    "\n",
    "# There are multiple functions that can create tensors, like torch.zeros(), torch.ones(), torch.rand()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tensor operations\n",
    "There are a lot.\n",
    "For instance, we can find the maximum value in a tensor like this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[0.2723, 0.4860],\n",
      "        [0.3239, 0.3828]])\n",
      "tensor(0.4860) tensor(1) 0.4860256314277649\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "print(x)\n",
    "print(x.max(), x.argmax(), x.max().item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are multiple types of tensors, for instance ```LongTensors``` or ```FloatTensors```. We can convert back and forth \n",
    "using the ```.to()``` method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "torch.LongTensor\n",
      "torch.FloatTensor\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "long_tensor = torch.tensor([[0, 0, 1], [1, 1, 1], [0, 0, 0]])\n",
    "print(long_tensor.type())\n",
    "float_tensor = long_tensor.to(dtype=torch.float32)\n",
    "print(float_tensor.type())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sometimes it may be useful to make use of **in-place** operations, as it will save memory by avoiding copying the tensor.\n",
    "In-place functions are post-fixed with a \"_\" symbol."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[0.7694, 0.5596],\n",
      "        [0.7815, 0.8139]])\n",
      "tensor([[0.7694, 0.5596],\n",
      "        [0.7815, 0.8139]])\n",
      "tensor([[-0.3782, -0.8375],\n",
      "        [-0.3556, -0.2970]])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "print(x)\n",
    "x.log2()\n",
    "print(x)\n",
    "x.log2_()  # Only this in-place operation will change the original tensor (x)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Reshaping\n",
    "An important task is to reshape tensors. We can use ```torch.view()``` or ```torch.reshape()```. The main difference is\n",
    "that ```torch.view()``` operates as a view of the original tensor, so if the underlying data is changed, the view will also\n",
    "change, whereas this does not happen with ```torch.reshape()```. Another difference is that ```torch.view()``` requires\n",
    "that the tensors/views being operated on **are contiguous**, that is, they need to share the same memory blocks they would\n",
    "occupy if a new tensor of the desired shape was created ex-novo. In this case, de-fragment stuff with ```torch.contiguous()```\n",
    "before the ```torch.view()``` operations.\n",
    "#### Rearranging tensor dimensions\n",
    "Another important operation is to re-arrange the dimensions in tensors. For instance, usually RGB image data is organized\n",
    "in ```[width, height, channel]``` but usually PyTorch likes this data as ```[channel, width, height]```. We can use the\n",
    "```torch.permute()``` method by supplying the new order of the dimensions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "torch.Size([3, 640, 480])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "x = torch.rand(640, 480, 3)\n",
    "x_rearranged = x.permute(2, 0, 1)  # put the last dimension (RGB) as the first one.\n",
    "print(x_rearranged.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tensor broadcasting\n",
    "Tensor broadcasting is an approach that allows to perform operations between a tensor and a smaller tensor. It is possible\n",
    "to broadcast across two tensors if, starting from their trailing dimensions:\n",
    "* The two dimensions are equal\n",
    "* One of the dimensions is 1\n",
    "\n",
    "The book doesn't really expand much on this, but my understanding is that, as long as the limitations above are respected,\n",
    "it can automatically pad the smaller tensor to have the same size as the larger one, and then operate.\n",
    "## Chapter 2. Image Classification with PyTorch\n",
    "Now we are going to incrementally build a simple neural network with the task of performing image classification between\n",
    "fishes and cats. First of all, we need data. The ```download.py``` script, included in the book's GitHub, supposedly\n",
    "downloads a subset of ImageNet data, already separated in 3 datasets (**training**, **test** and **validation**) and for\n",
    "each of these datasets, the images are already divided into fish or cat categories. The script, unfortunately, seems to \n",
    "have failed for several images, and several other had not been downloaded properly, so let's see how it goes. I guess it's\n",
    "a really real-world example...\n",
    "### PyTorch and Data Loaders\n",
    "Formally, a PyTorch ```dataset``` is a Python class that allows us to get at the data we're supplying to the neural network. \n",
    "A ```data loader``` is what actually feeds data from the dataset into the network. \n",
    "A dataset is defined as a class that defines at least a ```.__getitem__(self, index)``` method, and a ```.__len__(self)```\n",
    "method. These two methods provide a way of retrieving elements from the data, in ```(label, tensor)``` pairs, and a way\n",
    "of obtaining the size of the dataset, respectively.\n",
    "### Building a training dataset\n",
    "The ```torchvision``` module provide several convenience function to deal with image dataset, such as the\n",
    "```ImageFolder``` class, which will greatly simplify dealing with image data, as long as the images are contained in a \n",
    "directory structure where each directory is a label (i.e. ```./train/cat/```, ```./train/fish/``` etc).\n",
    "\n",
    "For our purposes, this will be enough:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_data_path = \"./book_examples/train/\"\n",
    "\n",
    "my_transforms = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=my_transforms)\n",
    "\n",
    "val_data_path = \"./book_examples/val/\"\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path, transform=my_transforms)\n",
    "\n",
    "test_data_path = \"./book_examples/test/\"\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path, transform=my_transforms)\n",
    "\n",
    "my_batch_size = 64\n",
    "train_data_loader = data.DataLoader(train_data, batch_size=my_batch_size)\n",
    "val_data_loader = data.DataLoader(val_data, batch_size=my_batch_size)\n",
    "test_data_loader = data.DataLoader(test_data, batch_size=my_batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that set ```datasets``` (and pertinent transforms) and ```dataloaders```, it's time to create the actual neural \n",
    "network!\n",
    "### Creating a network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Had to readjust several variables/imports that are not mentioned in the book examples. Cross-referencing with the official\n",
    "# 60-min tutorial helps a lot. (Yeah, that's mentioned in the book errata webpage...)\n",
    "# class SimpleNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(12288, 84)\n",
    "#         self.fc2 = nn.Linear(84, 50)\n",
    "#         self.fc3 = nn.Linear(50, 2)\n",
    "#     \n",
    "#     def forward(self,x ):\n",
    "#         x = x.view(-1, 12288)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.softmax(self.fc3(x))\n",
    "#         return x\n",
    "# \n",
    "# simplenet = SimpleNet()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Activation functions\n",
    "Activation functions define how the ouptut from one unit should be propagated to the post-synaptic units. A very commonly\n",
    "used activation function is the **```ReLU```** function, acronym for **rectified linear unit**. It basically implements\n",
    "```max(0, x)``` where ```x``` is the value being propagated. Another useful activation function is called **```softmax```**,\n",
    "and tries to \"exaggerate\" differences between values while keeping everything normalized so that it adds up to 1. This\n",
    "is explained pretty poorly but I'm not going to open a new browser tab for every ill-explained paragraph in the book.\n",
    "\n",
    "In the case of our ```simplenet``` implementation, we start by defining a initialization function. The first instruction\n",
    "calls the super class (```nn.Module```) initialization function which we inherit. Then, we define **three fully connected\n",
    "layers**, called **Linear** in PyTorch. We then define the ```forward()``` method, which defines how data is propagated\n",
    "through the network. We start by converting the input 3D tensor into a 1D tensor that is fed into the first Linear layer.\n",
    "Then, we apply the layers and respective activation functions in order, and then we return the ```softmax``` output to\n",
    "obtain the prediction from our network. \n",
    "\n",
    "The numbers defining the layers (i.e. inputs to the layer, outputs of the layer) are arbitrary with the exception of the \n",
    "final Linear layer (it needs to have 2 outputs, representing our two classes of stimuli). The idea is that we start with\n",
    "a high dimensionality in inputs, and as we progress through layers, we operate with less and less units, with the hope that\n",
    "the network, forced to work with progressively shrunk representations, will extract features available in the higher-level,\n",
    "lower-dimensionality representations. \n",
    "\n",
    "### Loss functions\n",
    "Loss functions define ways of computing how far is a prediction from the ground truth, and as such are essential for\n",
    "deep learning. PyTorch offers several different types of loss functions. For these examples, we are going to use a build-in\n",
    "loss function called **```CrossEntropyLoss```**, suitable for multi-class categorization tasks. Another common loss function\n",
    "is **```MSELoss```**, implementing the standard mean squared loss. An important point is that ```CrossEntropyLoss``` automatically\n",
    "applies ```softmax()``` as part of its use, so we need to account for this in out simplenet implementation. \n",
    "\n",
    "Just realized that they don't provide an example implementation that could have helped in the part dealing with implementing\n",
    "the actual learning loop. I'm going to try and just create an instance of a ```CrossEntropyLoss``` loss function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "my_loss_fn = nn.CrossEntropyLoss()\n",
    "print(type(my_loss_fn))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(12288, 84)\n",
    "        self.fc2 = nn.Linear(84, 50)\n",
    "        self.fc3 = nn.Linear(50, 2)\n",
    "    \n",
    "    def forward(self,x ):\n",
    "        x = x.view(-1, 12288)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "simplenet = SimpleNet()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now focus on how the layers are updated during the network's training loop.\n",
    "### Optimizing\n",
    "Basically, we are talking about finding minima across very high-dimensional data. There are several algorithms and PyTorch\n",
    "offers built-in tools to ease this process. Some practical point to keep in mind:\n",
    "* We should try **not to get trapped in local minima**, which could result from using exceedingly small learning rates.\n",
    "* Using exceedingly large learning rate might cause our optimization to **never converge on a suitable solution** for our\n",
    "given weights, data and task.\n",
    "\n",
    "To avoid these problems, other approaches for optimizing neural networks learning. PyTorch includes ```SGD``` (stochastic\n",
    "gradient descent), ```AdaGrad```, ```RMSProp``` and ```Adam``` (which is what we'll use in these examples), among others.\n",
    "Adam is particularly desirable for deep learning because it uses independent learning rate per parameter, adapting the\n",
    "learning rate depending on the rate of change of the parameters. \n",
    "We can create an Adam-based optimizer instance with the following code (assuming ```import torch.optim as optim```)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "my_optim = optim.Adam(simplenet.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "We can now write a loop to perform the actual training on our network and data. The final implementation will look like\n",
    "the following."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# # Doesn't work either.\n",
    "# epochs = 2\n",
    "# \n",
    "# for epochs in range(epochs):\n",
    "#     # PyTorch operates on a batch-based logic\n",
    "#     for batch in train_data_loader:\n",
    "#         my_optim.zero_grad()  # always zero the initial gradients at the beginning of the inner iteration\n",
    "#         curr_input, curr_target = batch  # Unpack the current batch, comprising an input and its ground truth in terms of target\n",
    "#         curr_output = simplenet(curr_input)  # present our net's instance with the input and obtain its guess\n",
    "#         curr_loss = my_loss_fn(curr_output, curr_target)  # using our specific instance of loss function (CrossEntropyLoss), calculate the distance from the correct answer\n",
    "#         curr_loss.backward()  # Basically, compute how each weight of each unit in each layer has contributed to the distance of the net's current guess from the current correct answer\n",
    "#         my_optim.step()  # Adjust the weights throughout our net accordingly\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, that doesn't work, let's try with the provided full implementation, which should work."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def my_train(model: SimpleNet, optimizer: torch.optim.Adam, loss_fn: nn.CrossEntropyLoss, train_loader: data.DataLoader, \n",
    "             val_loader: data.DataLoader, epochs: int = 20, device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "\n",
    "    :param model: \n",
    "    :type model: SimpleNet\n",
    "    :param optimizer: \n",
    "    :type optimizer: torch.optim.Adam\n",
    "    :param loss_fn: \n",
    "    :type loss_fn: nn.CrossEntropyLoss\n",
    "    :param train_loader: \n",
    "    :type train_loader: data.DataLoader\n",
    "    :param val_loader: \n",
    "    :type val_loader: data.DataLoader\n",
    "    :param epochs: \n",
    "    :type epochs: int\n",
    "    :param device: \n",
    "    :type device: str\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        curr_epoch_train_loss = 0.0\n",
    "        curr_epoch_valid_loss = 0.0\n",
    "        model.train()\n",
    "        cnt = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            curr_in, curr_tgt = batch\n",
    "            curr_in = curr_in.to(device)\n",
    "            curr_tgt = curr_tgt.to(device)\n",
    "            curr_out = model(curr_in)\n",
    "            curr_loss = loss_fn(curr_out, curr_tgt)\n",
    "            curr_loss.backward()\n",
    "            optimizer.step()\n",
    "            curr_epoch_train_loss += curr_loss.data.item()\n",
    "            cnt += 1\n",
    "        curr_epoch_train_loss /= cnt\n",
    "\n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_tot = 0\n",
    "        for batch in val_loader:\n",
    "            curr_in, curr_tgt = batch\n",
    "            curr_in = curr_in.to(device)\n",
    "            curr_tgt = curr_tgt.to(device)\n",
    "            curr_out = model(curr_in)\n",
    "            curr_loss = loss_fn(curr_out, curr_tgt)\n",
    "            curr_epoch_valid_loss += curr_loss.data.item()\n",
    "            correct = torch.eq(torch.max(F.softmax(curr_out), dim=1)[1], curr_tgt.view(-1))\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_tot = correct.shape[0]\n",
    "        curr_epoch_valid_loss /= len(val_loader)\n",
    "\n",
    "        print(\"Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy: {.2f}\".format(epoch, curr_epoch_valid_loss, num_correct / num_tot))\n",
    "#         \n",
    "# my_train(simplenet, my_optim, my_loss_fn, train_data_loader, test_data_loader, device=\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, none of the above worked. I'll try to proceed with Chapter 3, hoping that accuracy increases since I couldn't figure\n",
    "out how to fix the broken examples.\n",
    "## Chapter 3. Convolutional Neural Networks\n",
    "Let's have a look at our first CNN (hopefully)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# # The following doesn't work either, hence why it's commented out.\n",
    "# class CNNNet(nn.Module):\n",
    "#     def __init__(self, num_classes: int = 2):\n",
    "#         super(CNNNet, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(64, 192, kernel_size=3, padding=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=3, padding=1),\n",
    "#             nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(256 * 6 * 6, 4096),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(4096, num_classes)\n",
    "#         )\n",
    "#         \n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "#     \n",
    "# my_cnn = CNNNet()\n",
    "# my_train(my_cnn, loss_fn=my_loss_fn, optimizer=my_optim, train_loader=train_data_loader, val_loader=test_data_loader, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}